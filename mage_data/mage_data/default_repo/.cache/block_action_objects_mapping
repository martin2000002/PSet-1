{"block_file": {"data_exporters/data_exporter.py:data_exporter:python:data exporter": {"content": "from mage_ai.data_preparation.shared.secrets import get_secret_value\nimport psycopg2\nimport json\nimport time\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom dateutil import parser as date_parser\n\n# --- CONFIGURACI\u00d3N DE REINTENTOS PARA POSTGRES ---\nMAX_DB_RETRIES = 3\nDB_RETRY_BACKOFF = 2\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\ndef get_db_connection_with_retry(db_params, logger):\n    \"\"\"\n    Conexi\u00f3n a Postgres con reintentos para errores transitorios.\n    \"\"\"\n    retries = 0\n    while retries < MAX_DB_RETRIES:\n        try:\n            conn = psycopg2.connect(**db_params)\n            return conn\n        except psycopg2.OperationalError as e:\n            retries += 1\n            wait = (2 ** retries) * DB_RETRY_BACKOFF\n            logger.warning(f\"[DB-RETRY] Error de conexi\u00f3n: {str(e)}. Reintento {retries}/{MAX_DB_RETRIES} en {wait}s\")\n            time.sleep(wait)\n    raise Exception(f\"[DB-FAIL] No se pudo conectar a Postgres tras {MAX_DB_RETRIES} reintentos.\")\n\n\n@data_exporter\ndef export_data_to_postgres(df, *args, **kwargs):\n    \"\"\"\n    EXPORTER: Carga datos a PostgreSQL con:\n    - Idempotencia (UPSERT)\n    - Metadatos obligatorios\n    - Tracking insert/update/omitidos\n    - Validaci\u00f3n de coherencia temporal\n    - Reintentos para errores transitorios\n    - Persistencia de checkpoint para reanudaci\u00f3n\n    \"\"\"\n    logger = kwargs.get('logger')\n    start_time_load = time.time()\n    \n    # ========== 1. IDENTIFICACI\u00d3N DE ENTIDAD (Requisito 3 y 7.3) ==========\n    entity = kwargs.get('entity', 'Invoice')  # Invoice, Customer, Item\n    table_name = f\"qb_{entity.lower()}\"\n    schema_name = \"raw\"\n    \n    logger.info(f\"[CONFIG] Entidad: {entity} -> Tabla destino: {schema_name}.{table_name}\")\n\n    if df is None or df.empty:\n        logger.warning(f\"[VOLUMETRY] No hay datos para la entidad {table_name}. Fin de ejecuci\u00f3n.\")\n        return\n\n    # ========== 2. GESTI\u00d3N DE SECRETOS (Requisito 6) ==========\n    try:\n        db_params = {\n            'host': get_secret_value('POSTGRES_HOST'),\n            'database': get_secret_value('POSTGRES_DB'),\n            'user': get_secret_value('POSTGRES_USER'),\n            'password': get_secret_value('POSTGRES_PASSWORD'),\n            'port': get_secret_value('POSTGRES_PORT')\n        }\n        conn = get_db_connection_with_retry(db_params, logger)\n        cur = conn.cursor()\n    except Exception as e:\n        logger.error(f\"[SECURITY/DB] Error al obtener secretos o conectar a Postgres: {str(e)}\")\n        raise e\n\n    # ========== 3. DISE\u00d1O DE CAPA RAW (Requisito 7.3) ==========\n    try:\n        # Iniciar transacci\u00f3n expl\u00edcita para DDL (Requisito 7: robustez)\n        cur.execute(\"BEGIN;\")\n        cur.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema_name};\")\n        \n        # Tabla principal con Primary Key y Metadatos Obligatorios\n        create_table_query = f\"\"\"\n            CREATE TABLE IF NOT EXISTS {schema_name}.{table_name} (\n                id VARCHAR PRIMARY KEY,\n                payload JSONB,\n                ingested_at_utc TIMESTAMP WITH TIME ZONE,\n                extract_window_start_utc TIMESTAMP WITH TIME ZONE,\n                extract_window_end_utc TIMESTAMP WITH TIME ZONE,\n                page_number INT,\n                page_size INT,\n                request_payload TEXT,\n                source_last_updated_utc TIMESTAMP WITH TIME ZONE\n            );\n        \"\"\"\n        cur.execute(create_table_query)\n        conn.commit()\n        logger.info(f\"[DDL] \u2705 Tabla {schema_name}.{table_name} creada/verificada exitosamente.\")\n    except Exception as e:\n        logger.error(f\"[DDL] Error creando infraestructura RAW: {str(e)}\")\n        conn.rollback()\n        raise e\n\n    # ========== 4. CONTEO PREVIO PARA M\u00c9TRICAS (Requisito 7.5) ==========\n    try:\n        cur.execute(f\"SELECT COUNT(*) FROM {schema_name}.{table_name}\")\n        count_before = cur.fetchone()[0]\n    except:\n        count_before = 0\n\n    # ========== 5. IDEMPOTENCIA MEDIANTE UPSERT (Requisito 7.3 y 7.4) ==========\n    # Incluye page_size en el UPDATE para consistencia completa\n    upsert_sql = f\"\"\"\n        INSERT INTO {schema_name}.{table_name} (\n            id, payload, ingested_at_utc, extract_window_start_utc, \n            extract_window_end_utc, page_number, page_size, request_payload,\n            source_last_updated_utc\n        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n        ON CONFLICT (id) DO UPDATE SET\n            payload = EXCLUDED.payload,\n            ingested_at_utc = EXCLUDED.ingested_at_utc,\n            extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n            extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n            page_number = EXCLUDED.page_number,\n            page_size = EXCLUDED.page_size,\n            request_payload = EXCLUDED.request_payload,\n            source_last_updated_utc = EXCLUDED.source_last_updated_utc;\n    \"\"\"\n\n    rows_processed = 0\n    rows_with_temporal_issues = 0\n    rows_skipped_null_id = 0\n    \n    # ========== M\u00c9TRICAS POR TRAMO (Requisito 7.4 y 7.5) ==========\n    chunk_metrics = {}  # {chunk_key: {'count': N, 'window_start': X, 'window_end': Y}}\n    \n    try:\n        for _, row in df.iterrows():\n            # ========== VALIDACI\u00d3N DE INTEGRIDAD: PK NO NULA (Requisito 7.4) ==========\n            if not row['id'] or pd.isna(row['id']):\n                logger.error(f\"[VALIDATION] \u274c Registro con ID nulo omitido en exporter.\")\n                rows_skipped_null_id += 1\n                continue\n            \n            # ========== TRACKING POR TRAMO (Requisito 7.4) ==========\n            chunk_key = f\"{row['extract_window_start_utc']}|{row['extract_window_end_utc']}\"\n            if chunk_key not in chunk_metrics:\n                chunk_metrics[chunk_key] = {\n                    'count': 0,\n                    'window_start': row['extract_window_start_utc'],\n                    'window_end': row['extract_window_end_utc']\n                }\n            chunk_metrics[chunk_key]['count'] += 1\n            \n            # ========== 6. VALIDACI\u00d3N DE COHERENCIA TEMPORAL (Requisito 7.4) ==========\n            ingested_at = row['ingested_at_utc']\n            window_end_str = row['extract_window_end_utc']\n            source_updated = row.get('source_last_updated_utc', '')\n            \n            # Parsear window_end para comparaci\u00f3n\n            try:\n                window_end_dt = date_parser.parse(window_end_str) if isinstance(window_end_str, str) else window_end_str\n                if ingested_at.tzinfo is None:\n                    ingested_at = ingested_at.replace(tzinfo=timezone.utc)\n                if window_end_dt.tzinfo is None:\n                    window_end_dt = window_end_dt.replace(tzinfo=timezone.utc)\n                    \n                # Verificar coherencia: ingested_at debe ser >= window_end\n                if ingested_at < window_end_dt:\n                    rows_with_temporal_issues += 1\n                    logger.debug(f\"[TEMPORAL-WARNING] Registro {row['id']}: ingested_at < extract_window_end\")\n            except Exception as parse_error:\n                logger.debug(f\"[TEMPORAL-PARSE] No se pudo validar temporalidad para {row['id']}: {parse_error}\")\n            \n            # Parsear source_last_updated_utc\n            source_updated_ts = None\n            if source_updated:\n                try:\n                    source_updated_ts = date_parser.parse(source_updated)\n                except:\n                    source_updated_ts = None\n            \n            # Ejecutar UPSERT con reintentos\n            retry_count = 0\n            while retry_count < MAX_DB_RETRIES:\n                try:\n                    cur.execute(upsert_sql, (\n                        str(row['id']), \n                        json.dumps(row['payload']), \n                        row['ingested_at_utc'],\n                        row['extract_window_start_utc'], \n                        row['extract_window_end_utc'],\n                        row['page_number'], \n                        row['page_size'], \n                        row['request_payload'],\n                        source_updated_ts\n                    ))\n                    rows_processed += 1\n                    break\n                except psycopg2.OperationalError as e:\n                    retry_count += 1\n                    if retry_count >= MAX_DB_RETRIES:\n                        raise e\n                    logger.warning(f\"[DB-RETRY] Error en INSERT, reintentando... {retry_count}/{MAX_DB_RETRIES}\")\n                    time.sleep(DB_RETRY_BACKOFF * retry_count)\n                    # Reconectar si es necesario\n                    try:\n                        conn = get_db_connection_with_retry(db_params, logger)\n                        cur = conn.cursor()\n                    except:\n                        pass\n        \n        conn.commit()\n        logger.info(f\"[LOAD] Upsert exitoso: {rows_processed} filas procesadas en {table_name}.\")\n        \n    except Exception as e:\n        logger.error(f\"[LOAD] Fallo en la carga de datos: {str(e)}\")\n        conn.rollback()\n        raise e\n\n    # ========== 7. M\u00c9TRICAS GRANULARES: INSERT VS UPDATE VS OMITIDOS (Requisito 7.5) ==========\n    try:\n        cur.execute(f\"SELECT COUNT(*) FROM {schema_name}.{table_name}\")\n        count_after = cur.fetchone()[0]\n        \n        # C\u00e1lculo de m\u00e9tricas granulares\n        new_inserts = count_after - count_before\n        updates = rows_processed - new_inserts if rows_processed > new_inserts else 0\n        # Si hay menos inserts que rows_processed, la diferencia son updates\n        if new_inserts < 0:\n            new_inserts = 0\n            updates = rows_processed\n        omitted = len(df) - rows_processed\n        \n        logger.info(\"--- REPORTE DE CALIDAD (Requisito 7.4) ---\")\n        logger.info(f\"[QUALITY] Entidad: {table_name}\")\n        logger.info(f\"[QUALITY] Registros en DataFrame: {len(df)}\")\n        logger.info(f\"[QUALITY] Total registros en tabla (antes): {count_before}\")\n        logger.info(f\"[QUALITY] Total registros en tabla (despu\u00e9s): {count_after}\")\n        logger.info(f\"[METRICS-GRANULAR] Nuevas inserciones: {new_inserts}\")\n        logger.info(f\"[METRICS-GRANULAR] Actualizaciones (updates): {updates}\")\n        logger.info(f\"[METRICS-GRANULAR] Omitidos/errores: {omitted}\")\n        logger.info(f\"[IDEMPOTENCY] Upsert aplicado correctamente sobre PK 'id'.\")\n        \n        if rows_with_temporal_issues > 0:\n            logger.warning(f\"[TEMPORAL-QUALITY] \u26a0\ufe0f {rows_with_temporal_issues} registros con posibles \"\n                           f\"inconsistencias temporales (ingested_at < extract_window_end).\")\n        \n        if rows_skipped_null_id > 0:\n            logger.warning(f\"[INTEGRITY] \u26a0\ufe0f {rows_skipped_null_id} registros omitidos por ID nulo.\")\n        \n        # ========== REPORTE DE VOLUMETR\u00cdA POR TRAMO (Requisito 7.4) ==========\n        logger.info(\"--- VOLUMETR\u00cdA POR TRAMO (Requisito 7.4) ---\")\n        for chunk_key, metrics in chunk_metrics.items():\n            logger.info(f\"[CHUNK-VOLUMETRY] Ventana: [{metrics['window_start']} - {metrics['window_end']}] | \"\n                        f\"Registros: {metrics['count']}\")\n            # Detecci\u00f3n de d\u00edas vac\u00edos\n            if metrics['count'] == 0:\n                logger.warning(f\"[VOLUMETRY] \u26a0\ufe0f ALERTA: Tramo vac\u00edo detectado: {chunk_key}\")\n        logger.info(f\"[VOLUMETRY] Total tramos procesados: {len(chunk_metrics)}\")\n        \n        # Detecci\u00f3n de anomal\u00edas (Requisito 7.4)\n        if rows_processed == 0 and not df.empty:\n            logger.warning(\"[QUALITY] \u26a0\ufe0f ALERTA: El DataFrame ten\u00eda datos pero no se proces\u00f3 nada en Postgres.\")\n\n    except Exception as e:\n        logger.warning(f\"[QUALITY] No se pudo generar reporte de volumetr\u00eda: {str(e)}\")\n\n    # ========== 8. ESTADO DEL PIPELINE (Requisito 7.5) ==========\n    # Nota: El checkpoint para reanudaci\u00f3n se muestra en el Loader donde ocurre el error\n    try:\n        pipeline_failed = False\n        if hasattr(df, 'attrs'):\n            pipeline_failed = df.attrs.get('pipeline_failed', False)\n        \n        if pipeline_failed:\n            logger.warning(f\"[EXPORTER] \u26a0\ufe0f Datos de extracci\u00f3n parcial exportados exitosamente.\")\n            logger.warning(f\"[EXPORTER] Revisar logs del Loader para instrucciones de reanudaci\u00f3n.\")\n        else:\n            logger.info(f\"[EXPORTER] \u2705 Pipeline completado exitosamente.\")\n    except Exception as e:\n        logger.warning(f\"[EXPORTER] No se pudo verificar estado del pipeline: {str(e)}\")\n    \n    finally:\n        cur.close()\n        conn.close()\n\n    # ========== 9. OBSERVABILIDAD Y M\u00c9TRICAS FINALES (Requisito 7.5) ==========\n    duration = round(time.time() - start_time_load, 2)\n    logger.info(\"--- M\u00c9TRICAS DE OPERACI\u00d3N (Requisito 7.5) ---\")\n    logger.info(f\"Fase: CARGA RAW\")\n    logger.info(f\"Registros procesados: {rows_processed}\")\n    logger.info(f\"Nuevos: {new_inserts} | Actualizados: {updates} | Omitidos: {omitted}\")\n    logger.info(f\"Duraci\u00f3n: {duration} segundos\")\n    logger.info(f\"Coherencia Temporal: Marcas registradas en UTC\")\n    logger.info(\"--------------------------------------------\")", "file_path": "data_exporters/data_exporter.py", "language": "python", "type": "data_exporter", "uuid": "data_exporter"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_exporters/invoices_data_exporter.py:data_exporter:python:invoices data exporter": {"content": "from mage_ai.data_preparation.shared.secrets import get_secret_value\nimport psycopg2\nimport json\nimport time\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom dateutil import parser as date_parser\n\nMAX_DB_RETRIES = 3\nDB_RETRY_BACKOFF = 2\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\ndef get_db_connection_with_retry(db_params, logger):\n    retries = 0\n    while retries < MAX_DB_RETRIES:\n        try:\n            conn = psycopg2.connect(**db_params)\n            return conn\n        except psycopg2.OperationalError as e:\n            retries += 1\n            wait = (2 ** retries) * DB_RETRY_BACKOFF\n            logger.warning(f\"[DB-RETRY] Error de conexi\u00f3n: {str(e)}. Reintento {retries}/{MAX_DB_RETRIES} en {wait}s\")\n            time.sleep(wait)\n    raise Exception(f\"[DB-FAIL] No se pudo conectar a Postgres tras {MAX_DB_RETRIES} reintentos.\")\n\n\n@data_exporter\ndef export_data_to_postgres(df, *args, **kwargs):\n    logger = kwargs.get('logger')\n    start_time_load = time.time()\n    \n    entity = 'Invoice'\n    table_name = f\"qb_{entity.lower()}\"\n    schema_name = \"raw\"\n    \n    if df is None or df.empty:\n        logger.warning(f\"[VOLUMETRY] No hay datos para la entidad {table_name}. Fin de ejecuci\u00f3n.\")\n        return\n\n    try:\n        db_params = {\n            'host': get_secret_value('POSTGRES_HOST'),\n            'database': get_secret_value('POSTGRES_DB'),\n            'user': get_secret_value('POSTGRES_USER'),\n            'password': get_secret_value('POSTGRES_PASSWORD'),\n            'port': get_secret_value('POSTGRES_PORT')\n        }\n        conn = get_db_connection_with_retry(db_params, logger)\n        cur = conn.cursor()\n    except Exception as e:\n        logger.error(f\"[SECURITY/DB] Error al obtener secretos o conectar a Postgres: {str(e)}\")\n        raise e\n\n    try:\n        cur.execute(\"BEGIN;\")\n        cur.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema_name};\")\n        \n        create_table_query = f\"\"\"\n            CREATE TABLE IF NOT EXISTS {schema_name}.{table_name} (\n                id VARCHAR PRIMARY KEY,\n                payload JSONB,\n                ingested_at_utc TIMESTAMP WITH TIME ZONE,\n                extract_window_start_utc TIMESTAMP WITH TIME ZONE,\n                extract_window_end_utc TIMESTAMP WITH TIME ZONE,\n                page_number INT,\n                page_size INT,\n                request_payload TEXT,\n                source_last_updated_utc TIMESTAMP WITH TIME ZONE\n            );\n        \"\"\"\n        cur.execute(create_table_query)\n        conn.commit()\n        logger.info(f\"[DDL] Tabla {schema_name}.{table_name} creada/verificada exitosamente.\")\n    except Exception as e:\n        logger.error(f\"[DDL] Error creando infraestructura RAW: {str(e)}\")\n        conn.rollback()\n        raise e\n\n    try:\n        cur.execute(f\"SELECT COUNT(*) FROM {schema_name}.{table_name}\")\n        count_before = cur.fetchone()[0]\n    except:\n        count_before = 0\n\n    upsert_sql = f\"\"\"\n        INSERT INTO {schema_name}.{table_name} (\n            id, payload, ingested_at_utc, extract_window_start_utc, \n            extract_window_end_utc, page_number, page_size, request_payload,\n            source_last_updated_utc\n        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n        ON CONFLICT (id) DO UPDATE SET\n            payload = EXCLUDED.payload,\n            ingested_at_utc = EXCLUDED.ingested_at_utc,\n            extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n            extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n            page_number = EXCLUDED.page_number,\n            page_size = EXCLUDED.page_size,\n            request_payload = EXCLUDED.request_payload,\n            source_last_updated_utc = EXCLUDED.source_last_updated_utc;\n    \"\"\"\n\n    rows_processed = 0\n    rows_with_temporal_issues = 0\n    rows_skipped_null_id = 0\n    chunk_metrics = {}\n    \n    try:\n        for _, row in df.iterrows():\n            if not row['id'] or pd.isna(row['id']):\n                logger.error(f\"[VALIDATION] Registro con ID nulo omitido en exporter.\")\n                rows_skipped_null_id += 1\n                continue\n            \n            chunk_key = f\"{row['extract_window_start_utc']}|{row['extract_window_end_utc']}\"\n            if chunk_key not in chunk_metrics:\n                chunk_metrics[chunk_key] = {\n                    'count': 0,\n                    'window_start': row['extract_window_start_utc'],\n                    'window_end': row['extract_window_end_utc']\n                }\n            chunk_metrics[chunk_key]['count'] += 1\n            \n            ingested_at = row['ingested_at_utc']\n            window_end_str = row['extract_window_end_utc']\n            source_updated = row.get('source_last_updated_utc', '')\n            \n            try:\n                window_end_dt = date_parser.parse(window_end_str) if isinstance(window_end_str, str) else window_end_str\n                if ingested_at.tzinfo is None:\n                    ingested_at = ingested_at.replace(tzinfo=timezone.utc)\n                if window_end_dt.tzinfo is None:\n                    window_end_dt = window_end_dt.replace(tzinfo=timezone.utc)\n                    \n                if ingested_at < window_end_dt:\n                    rows_with_temporal_issues += 1\n                    logger.debug(f\"[TEMPORAL-WARNING] Registro {row['id']}: ingested_at < extract_window_end\")\n            except Exception as parse_error:\n                logger.debug(f\"[TEMPORAL-PARSE] No se pudo validar temporalidad para {row['id']}: {parse_error}\")\n            \n            source_updated_ts = None\n            if source_updated:\n                try:\n                    source_updated_ts = date_parser.parse(source_updated)\n                except:\n                    source_updated_ts = None\n            \n            retry_count = 0\n            while retry_count < MAX_DB_RETRIES:\n                try:\n                    cur.execute(upsert_sql, (\n                        str(row['id']), \n                        json.dumps(row['payload']), \n                        row['ingested_at_utc'],\n                        row['extract_window_start_utc'], \n                        row['extract_window_end_utc'],\n                        row['page_number'], \n                        row['page_size'], \n                        row['request_payload'],\n                        source_updated_ts\n                    ))\n                    rows_processed += 1\n                    break\n                except psycopg2.OperationalError as e:\n                    retry_count += 1\n                    if retry_count >= MAX_DB_RETRIES:\n                        raise e\n                    logger.warning(f\"[DB-RETRY] Error en INSERT, reintentando... {retry_count}/{MAX_DB_RETRIES}\")\n                    time.sleep(DB_RETRY_BACKOFF * retry_count)\n\n                    try:\n                        conn = get_db_connection_with_retry(db_params, logger)\n                        cur = conn.cursor()\n                    except:\n                        pass\n        \n        conn.commit()\n        logger.info(f\"[LOAD] Upsert exitoso: {rows_processed} filas procesadas en {table_name}.\")\n        \n    except Exception as e:\n        logger.error(f\"[LOAD] Fallo en la carga de datos: {str(e)}\")\n        conn.rollback()\n        raise e\n\n    # Metricas\n    try:\n        cur.execute(f\"SELECT COUNT(*) FROM {schema_name}.{table_name}\")\n        count_after = cur.fetchone()[0]\n        \n        new_inserts = count_after - count_before\n        updates = rows_processed - new_inserts if rows_processed > new_inserts else 0\n\n        if new_inserts < 0:\n            new_inserts = 0\n            updates = rows_processed\n        omitted = len(df) - rows_processed\n        \n        logger.info(\"--- REPORTE DE CALIDAD ---\")\n        logger.info(f\"[QUALITY] Entidad: {table_name}\")\n        logger.info(f\"[QUALITY] Registros en DataFrame: {len(df)}\")\n        logger.info(f\"[QUALITY] Total registros en tabla (antes): {count_before}\")\n        logger.info(f\"[QUALITY] Total registros en tabla (despu\u00e9s): {count_after}\")\n        logger.info(f\"[METRICS-GRANULAR] Nuevas inserciones: {new_inserts}\")\n        logger.info(f\"[METRICS-GRANULAR] Actualizaciones (updates): {updates}\")\n        logger.info(f\"[METRICS-GRANULAR] Omitidos/errores: {omitted}\")\n        logger.info(f\"[IDEMPOTENCY] Upsert aplicado correctamente sobre PK 'id'.\")\n        \n        if rows_with_temporal_issues > 0:\n            logger.warning(f\"[TEMPORAL-QUALITY] {rows_with_temporal_issues} registros con posibles \"\n                           f\"inconsistencias temporales (ingested_at < extract_window_end).\")\n        \n        if rows_skipped_null_id > 0:\n            logger.warning(f\"[INTEGRITY] {rows_skipped_null_id} registros omitidos por ID nulo.\")\n        \n        logger.info(\"--- VOLUMETR\u00cdA POR TRAMO ---\")\n        for chunk_key, metrics in chunk_metrics.items():\n            logger.info(f\"[CHUNK-VOLUMETRY] Ventana: [{metrics['window_start']} - {metrics['window_end']}] | \"\n                        f\"Registros: {metrics['count']}\")\n\n            if metrics['count'] == 0:\n                logger.warning(f\"[VOLUMETRY] ALERTA: Tramo vac\u00edo detectado: {chunk_key}\")\n        logger.info(f\"[VOLUMETRY] Total tramos procesados: {len(chunk_metrics)}\")\n        \n        if rows_processed == 0 and not df.empty:\n            logger.warning(\"[QUALITY] ALERTA: El DataFrame ten\u00eda datos pero no se proces\u00f3 nada en Postgres.\")\n\n    except Exception as e:\n        logger.warning(f\"[QUALITY] No se pudo generar reporte de volumetr\u00eda: {str(e)}\")\n\n    try:\n        pipeline_failed = False\n        if hasattr(df, 'attrs'):\n            pipeline_failed = df.attrs.get('pipeline_failed', False)\n        \n        if pipeline_failed:\n            logger.warning(f\"[EXPORTER] Datos de extracci\u00f3n parcial exportados exitosamente.\")\n            logger.warning(f\"[EXPORTER] Revisar logs del Loader para instrucciones de reanudaci\u00f3n.\")\n        else:\n            logger.info(f\"[EXPORTER] Pipeline completado exitosamente.\")\n    except Exception as e:\n        logger.warning(f\"[EXPORTER] No se pudo verificar estado del pipeline: {str(e)}\")\n    \n    finally:\n        cur.close()\n        conn.close()\n\n    # Resumen final\n    duration = round(time.time() - start_time_load, 2)\n    logger.info(\"--- RESUMEN FINAL ---\")\n    logger.info(f\"Registros procesados: {rows_processed}\")\n    logger.info(f\"Nuevos: {new_inserts} | Actualizados: {updates} | Omitidos: {omitted}\")\n    logger.info(f\"Duraci\u00f3n: {duration} segundos\")\n    logger.info(f\"Coherencia Temporal: Marcas registradas en UTC\")\n    logger.info(\"--------------------------------------------\")", "file_path": "data_exporters/invoices_data_exporter.py", "language": "python", "type": "data_exporter", "uuid": "invoices_data_exporter"}, "data_loaders/data_loader.py:data_loader:python:data loader": {"content": "from mage_ai.data_preparation.shared.secrets import get_secret_value\nimport requests\nimport base64\nimport time\nimport pandas as pd\nfrom datetime import datetime, timedelta, timezone\nfrom dateutil import parser as date_parser\n\n# --- CONFIGURACI\u00d3N DE PAR\u00c1METROS ---\nCHUNK_DAYS = 1           # Tama\u00f1o del segmento (Requisito 7.1)\nPAGE_SIZE = 20           # Registros por petici\u00f3n (Requisito 7.2)\nMAX_RETRIES = 5          # Reintentos para Resiliencia\nINITIAL_BACKOFF = 5      # Segundos base para Backoff\nCOURTESY_WAIT = 0.5      # Pausa entre p\u00e1ginas (aumentado para QBO)\nCIRCUIT_BREAKER_THRESHOLD = 3  # Fallos consecutivos para activar circuit breaker\n\n# URLs din\u00e1micas seg\u00fan entorno (Requisito 6: sandbox/prod)\nQBO_URLS = {\n    'sandbox': \"https://sandbox-quickbooks.api.intuit.com/v3/company\",\n    'production': \"https://quickbooks.api.intuit.com/v3/company\"\n}\nTOKEN_URL = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\ndef get_new_access_token(client_id, client_secret, refresh_token, logger):\n    \"\"\"\n    Fase Auth: Refresh Token al inicio de cada TRAMO (Requisito 7.2)\n    IMPORTANTE: Captura y retorna el nuevo refresh_token para rotaci\u00f3n.\n    \"\"\"\n    logger.info(f\"[AUTH] Iniciando autenticaci\u00f3n OAuth 2.0...\")\n    \n    auth_header = base64.b64encode(f\"{client_id}:{client_secret}\".encode()).decode()\n    headers = {\n        'Authorization': f'Basic {auth_header}',\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'Accept': 'application/json'\n    }\n    payload = {'grant_type': 'refresh_token', 'refresh_token': refresh_token}\n    \n    response = requests.post(TOKEN_URL, headers=headers, data=payload)\n    if response.status_code != 200:\n        logger.error(f\"[AUTH] \u274c Error Cr\u00edtico OAuth: {response.text}\")\n        raise Exception(f\"OAuth Failure: {response.status_code}\")\n    \n    token_data = response.json()\n    access_token = token_data.get('access_token')\n    new_refresh_token = token_data.get('refresh_token')\n    \n    logger.info(f\"[AUTH] \u2705 Access Token obtenido exitosamente\")\n    \n    # Rotaci\u00f3n de Refresh Token: Log para actualizaci\u00f3n manual (SIN EXPONER EL TOKEN)\n    if new_refresh_token and new_refresh_token != refresh_token:\n        logger.warning(f\"[AUTH-ROTATION] \u26a0\ufe0f NUEVO REFRESH TOKEN EMITIDO.\")\n        logger.warning(f\"[AUTH-ROTATION] Actualizar secreto QBO_REFRESH_TOKEN en Mage Secrets.\")\n        # SEGURIDAD: No loguear el token, solo notificar que cambi\u00f3\n        logger.info(f\"[AUTH-ROTATION] Token rotado. Longitud: {len(new_refresh_token)} caracteres.\")\n    else:\n        logger.info(f\"[AUTH] Refresh Token sin cambios.\")\n    \n    return access_token, new_refresh_token\n\n@data_loader\ndef load_data_from_quickbooks(*args, **kwargs):\n    \"\"\"\n    LOADER: Extrae datos de QuickBooks Online API.\n    - Maneja chunks temporales\n    - Paginaci\u00f3n completa\n    - Circuit breaker global\n    - M\u00e9tricas por tramo\n    \"\"\"\n    logger = kwargs.get('logger')\n    \n    # ========== 1. PAR\u00c1METROS Y VALIDACIONES (Requisito 3 y 7.1) ==========\n    entity = kwargs.get('entity', 'Invoice')  # Invoice, Customer, Item\n    start_date_str = kwargs.get('fecha_inicio')\n    end_date_str = kwargs.get('fecha_fin')\n    resume_from_str = kwargs.get('resume_from')  # Para reanudaci\u00f3n (Requisito 7.5 Runbook)\n    \n    logger.info(f\"[CONFIG] Entidad a extraer: {entity}\")\n    \n    if not start_date_str or not end_date_str:\n        raise ValueError(\"[VALIDATION] Error: 'fecha_inicio' y 'fecha_fin' son obligatorios.\")\n\n    # Parseo correcto de fechas con conversi\u00f3n a UTC (FIX: usar astimezone, no replace)\n    # Esto maneja correctamente offsets como -08:00 (PST) convirti\u00e9ndolos a UTC\n    def parse_to_utc(date_str):\n        \"\"\"Parsea fecha ISO y la convierte correctamente a UTC.\"\"\"\n        # Usar dateutil para manejar cualquier formato ISO con offset\n        dt = date_parser.parse(date_str)\n        # Si no tiene timezone, asumir UTC\n        if dt.tzinfo is None:\n            dt = dt.replace(tzinfo=timezone.utc)\n        # Convertir a UTC (astimezone convierte, replace solo cambia la etiqueta)\n        return dt.astimezone(timezone.utc)\n    \n    dt_start = parse_to_utc(start_date_str)\n    dt_end = parse_to_utc(end_date_str)\n        \n    # Validaci\u00f3n: fecha_inicio < fecha_fin (prevenir bucle infinito)\n    if dt_start >= dt_end:\n        raise ValueError(f\"[VALIDATION] Error: 'fecha_inicio' ({dt_start}) debe ser anterior a 'fecha_fin' ({dt_end}).\")\n    \n    # Reanudaci\u00f3n desde checkpoint (Requisito 7.5 Runbook)\n    if resume_from_str:\n        dt_resume = parse_to_utc(resume_from_str)\n        if dt_resume > dt_start and dt_resume < dt_end:\n            logger.info(f\"[RESUME] Reanudando desde checkpoint: {resume_from_str}\")\n            dt_start = dt_resume\n    \n    # ========== 2. SECRETOS Y CONFIGURACI\u00d3N (Requisito 6) ==========\n    client_id = get_secret_value('QBO_CLIENT_ID')\n    client_secret = get_secret_value('QBO_CLIENT_SECRET')\n    refresh_token = get_secret_value('QBO_REFRESH_TOKEN')\n    realm_id = get_secret_value('QBO_REALM_ID')\n    qbo_environment = get_secret_value('QBO_ENVIRONMENT') or 'sandbox'  # sandbox/production\n    \n    # URL din\u00e1mica seg\u00fan entorno\n    qbo_base_url = QBO_URLS.get(qbo_environment.lower(), QBO_URLS['sandbox'])\n    logger.info(f\"[CONFIG] Entorno QBO: {qbo_environment} | URL Base: {qbo_base_url}\")\n    \n    # ========== 3. VARIABLES DE CONTROL ==========\n    all_final_records = []\n    current_date = dt_start\n    current_refresh_token = refresh_token\n    consecutive_failures = 0  # Para Circuit Breaker\n    total_start_time = time.time()\n    \n    # Metadatos para el exporter (checkpoint info)\n    last_successful_chunk_end = None\n    chunk_index = 0  # Contador de tramos para simulaci\u00f3n de fallos\n    pipeline_failed = False  # Flag para indicar si hubo error\n    original_fecha_fin = end_date_str  # Guardar fecha_fin original para comparar\n\n    # ========== 4. BUCLE EXTERNO: SEGMENTACI\u00d3N POR TRAMO (Requisito 7.1) ==========\n    # Agregar 1 segundo al final para incluir el \u00faltimo registro exacto (>= y <=)\n    dt_end_inclusive = dt_end + timedelta(seconds=1)\n    \n    while current_date < dt_end_inclusive:\n        chunk_index += 1\n        start_time_chunk = time.time()\n        next_date = min(current_date + timedelta(days=CHUNK_DAYS), dt_end_inclusive)\n        \n        chunk_start = current_date.strftime('%Y-%m-%dT%H:%M:%S+00:00')\n        chunk_end = next_date.strftime('%Y-%m-%dT%H:%M:%S+00:00')\n        \n        try:\n            # OAUTH 2.0: Refresh al inicio de cada TRAMO (Requisito 7.2)\n            access_token, new_refresh_token = get_new_access_token(\n                client_id, client_secret, current_refresh_token, logger\n            )\n            # Actualizar token para siguiente tramo si fue rotado\n            if new_refresh_token:\n                current_refresh_token = new_refresh_token\n            \n            logger.info(f\"[CHUNK] --- Iniciando Tramo: {chunk_start} a {chunk_end} ---\")\n            \n            start_position = 1\n            more_data_in_chunk = True\n            pages_in_chunk = 0\n            records_in_chunk = 0\n            \n            # ========== 5. BUCLE MEDIO: PAGINACI\u00d3N (Requisito 7.2) ==========\n            while more_data_in_chunk:\n                # Filtros hist\u00f3ricos en UTC (Requisito 7.2)\n                query = (f\"SELECT * FROM {entity} \"\n                         f\"WHERE Metadata.LastUpdatedTime >= '{chunk_start}' \"\n                         f\"AND Metadata.LastUpdatedTime < '{chunk_end}' \"\n                         f\"STARTPOSITION {start_position} MAXRESULTS {PAGE_SIZE}\")\n                \n                url = f\"{qbo_base_url}/{realm_id}/query\"\n                headers = {'Authorization': f'Bearer {access_token}', 'Accept': 'application/json'}\n                \n                # ========== 6. BUCLE INTERNO: RESILIENCIA CON BACKOFF (Requisito 7.2) ==========\n                retries = 0\n                success = False\n                response = None\n                \n                while retries < MAX_RETRIES and not success:\n                    try:\n                        response = requests.get(url, headers=headers, params={'query': query})\n                        \n                        if response.status_code == 200:\n                            success = True\n                            consecutive_failures = 0  # Reset circuit breaker\n                        elif response.status_code == 429:\n                            wait = (2 ** retries) * INITIAL_BACKOFF\n                            logger.warning(f\"[RATE-LIMIT] HTTP 429. Reintento {retries+1}/{MAX_RETRIES} en {wait}s\")\n                            time.sleep(wait)\n                            retries += 1\n                        elif response.status_code == 401:\n                            # Token expirado, refrescar (NO cuenta como reintento)\n                            logger.warning(\"[AUTH] Token expirado, refrescando...\")\n                            access_token, new_refresh_token = get_new_access_token(\n                                client_id, client_secret, current_refresh_token, logger\n                            )\n                            if new_refresh_token:\n                                current_refresh_token = new_refresh_token\n                            headers = {'Authorization': f'Bearer {access_token}', 'Accept': 'application/json'}\n                            # NO incrementar retries: refresh de token no es un error real\n                        else:\n                            logger.error(f\"[API-ERROR] HTTP {response.status_code}: {response.text}\")\n                            retries += 1\n                            time.sleep(INITIAL_BACKOFF)\n                    except requests.exceptions.RequestException as e:\n                        logger.error(f\"[NETWORK-ERROR] {str(e)}. Reintento {retries+1}/{MAX_RETRIES}\")\n                        retries += 1\n                        time.sleep((2 ** retries) * INITIAL_BACKOFF)\n\n                if not success:\n                    consecutive_failures += 1\n                    logger.error(f\"[CHUNK-FAIL] Tramo {chunk_start} fall\u00f3 despu\u00e9s de {MAX_RETRIES} reintentos.\")\n                    \n                    # ========== CIRCUIT BREAKER (Requisito 7.2) ==========\n                    if consecutive_failures >= CIRCUIT_BREAKER_THRESHOLD:\n                        logger.critical(f\"[CIRCUIT-BREAKER] \u26d4 {consecutive_failures} fallos consecutivos. \"\n                                        f\"Pipeline detenido. \u00daltimo tramo exitoso: {last_successful_chunk_end}\")\n                        raise Exception(f\"Circuit Breaker activado tras {consecutive_failures} fallos consecutivos.\")\n                    break  # Salir del bucle de paginaci\u00f3n para este tramo\n\n                # ========== 7. PROCESAMIENTO DE DATOS Y METADATOS (Requisito 7.3) ==========\n                data_payload = response.json().get('QueryResponse', {}).get(entity, [])\n                \n                for record in data_payload:\n                    # ========== VALIDACI\u00d3N DE INTEGRIDAD: PK NO NULA (Requisito 7.4) ==========\n                    record_id = record.get('Id')\n                    if not record_id:\n                        logger.error(f\"[VALIDATION] \u274c Registro sin ID omitido. Payload parcial: {str(record)[:200]}\")\n                        continue  # Omitir registros sin PK\n                    \n                    # Validaci\u00f3n de coherencia temporal del dato (Requisito 7.4)\n                    record_last_updated = record.get('MetaData', {}).get('LastUpdatedTime', '')\n                    \n                    all_final_records.append({\n                        'id': record.get('Id'),\n                        'payload': record,\n                        'ingested_at_utc': datetime.now(timezone.utc),\n                        'extract_window_start_utc': chunk_start,\n                        'extract_window_end_utc': chunk_end,\n                        'page_number': (start_position // PAGE_SIZE) + 1,\n                        'page_size': PAGE_SIZE,\n                        'request_payload': query,\n                        'source_last_updated_utc': record_last_updated  # Para validaci\u00f3n\n                    })\n                \n                pages_in_chunk += 1\n                records_in_chunk += len(data_payload)\n                \n                # Condici\u00f3n de salida de paginaci\u00f3n\n                if len(data_payload) < PAGE_SIZE:\n                    more_data_in_chunk = False\n                else:\n                    start_position += PAGE_SIZE\n                    time.sleep(COURTESY_WAIT)\n\n            # ========== 8. M\u00c9TRICAS Y VOLUMETR\u00cdA POR TRAMO (Requisito 7.1 y 7.5) ==========\n            duration_chunk = round(time.time() - start_time_chunk, 2)\n            \n            # Detecci\u00f3n de anomal\u00edas: d\u00edas vac\u00edos (Requisito 7.4)\n            if records_in_chunk == 0:\n                logger.warning(f\"[VOLUMETRY] \u26a0\ufe0f ALERTA: Tramo {chunk_start} a {chunk_end} retorn\u00f3 0 registros. \"\n                               f\"Verificar si es esperado o hay problema de filtros/datos.\")\n            \n            logger.info(f\"[METRICS] Tramo Finalizado: \"\n                        f\"Ventana: [{chunk_start} - {chunk_end}] | \"\n                        f\"P\u00e1ginas: {pages_in_chunk} | \"\n                        f\"Registros: {records_in_chunk} | \"\n                        f\"Duraci\u00f3n: {duration_chunk}s\")\n            \n            # Actualizar checkpoint para reanudaci\u00f3n\n            last_successful_chunk_end = chunk_end\n            \n        except Exception as e:\n            consecutive_failures += 1\n            pipeline_failed = True\n            logger.error(f\"[CHUNK-ERROR] \u274c Error en tramo #{chunk_index} ({chunk_start}): {str(e)}\")\n            \n            # Mostrar instrucciones de checkpoint para reanudaci\u00f3n\n            if last_successful_chunk_end:\n                logger.critical(f\"[CHECKPOINT] \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\")\n                logger.critical(f\"[CHECKPOINT] \u26d4 PIPELINE INTERRUMPIDO EN TRAMO #{chunk_index}\")\n                logger.critical(f\"[CHECKPOINT] \u00daltimo tramo exitoso: #{chunk_index - 1}\")\n                logger.critical(f\"[CHECKPOINT] \u00daltimo checkpoint: {last_successful_chunk_end}\")\n                logger.critical(f\"[CHECKPOINT] \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\")\n                logger.critical(f\"[CHECKPOINT] \ud83d\udccb PARA REANUDAR, usar par\u00e1metro:\")\n                logger.critical(f\"[CHECKPOINT]    resume_from = '{last_successful_chunk_end}'\")\n                logger.critical(f\"[CHECKPOINT] \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\")\n            else:\n                logger.critical(f\"[CHECKPOINT] \u26d4 PIPELINE FALL\u00d3 EN EL PRIMER TRAMO. No hay checkpoint disponible.\")\n            \n            # Salir del bucle pero retornar los datos ya extra\u00eddos (tramos exitosos)\n            logger.warning(f\"[RECOVERY] Retornando {len(all_final_records)} registros de tramos exitosos anteriores.\")\n            break\n\n        current_date = next_date\n\n    # ========== 9. RESUMEN FINAL Y METADATOS PARA EXPORTER ==========\n    total_duration = round(time.time() - total_start_time, 2)\n    \n    if pipeline_failed:\n        logger.warning(f\"[EXTRACTION-PARTIAL] === EXTRACCI\u00d3N PARCIAL (CON ERRORES) ===\")\n        logger.warning(f\"[EXTRACTION-PARTIAL] Tramos completados exitosamente: {chunk_index - 1}\")\n    else:\n        logger.info(f\"[EXTRACTION-COMPLETE] === EXTRACCI\u00d3N FINALIZADA EXITOSAMENTE ===\")\n    \n    logger.info(f\"[EXTRACTION-COMPLETE] Total registros: {len(all_final_records)}\")\n    logger.info(f\"[EXTRACTION-COMPLETE] Duraci\u00f3n total: {total_duration}s\")\n    logger.info(f\"[EXTRACTION-COMPLETE] Entidad: {entity}\")\n    logger.info(f\"[EXTRACTION-COMPLETE] Rango solicitado: {start_date_str} a {end_date_str}\")\n    if resume_from_str:\n        logger.info(f\"[EXTRACTION-COMPLETE] Reanudado desde checkpoint: {resume_from_str}\")\n    \n    if len(all_final_records) == 0:\n        logger.warning(\"[VOLUMETRY] \u26a0\ufe0f No se extrajeron registros. Verificar rango de fechas y datos en QBO.\")\n    \n    # Crear DataFrame con metadatos de checkpoint para el exporter\n    df = pd.DataFrame(all_final_records)\n    \n    # Agregar metadatos globales como atributos del DataFrame (para el exporter)\n    if not df.empty:\n        df.attrs['last_checkpoint'] = last_successful_chunk_end\n        df.attrs['pipeline_failed'] = pipeline_failed\n        df.attrs['original_fecha_fin'] = original_fecha_fin\n    \n    return df", "file_path": "data_loaders/data_loader.py", "language": "python", "type": "data_loader", "uuid": "data_loader"}, "data_loaders/honorable_flower.py:data_loader:python:honorable flower": {"content": "from mage_ai.orchestration.db import db_connection\nfrom mage_ai.orchestration.db.models import Secret\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport datetime\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n@data_loader\ndef test_hardcore_rotation(*args, **kwargs):\n    # 1. Par\u00e1metros de prueba\n    # ASEG\u00daRATE de haber creado este secreto en Settings -> Secrets antes\n    target_secret = 'TEST_ROTATION_TOKEN' \n    nuevo_valor = f\"TOKEN_GENERADO_{datetime.datetime.now().strftime('%H%M%S')}\"\n    \n    print(f\"--- INICIANDO PRUEBA DE ROTACI\u00d3N ---\")\n    print(f\"Buscando secreto: {target_secret}\")\n    \n    try:\n        # 2. Abrir sesi\u00f3n de base de datos interna de Mage\n        with db_connection.session_scope() as session:\n            print(\"Conexi\u00f3n a la DB de Mage abierta exitosamente.\")\n            \n            # 3. Buscar el registro del secreto\n            secret_record = session.query(Secret).filter(Secret.name == target_secret).first()\n            \n            if secret_record:\n                print(f\"Secreto encontrado. Valor actual: {secret_record.value}\")\n                print(f\"Intentando cambiar a: {nuevo_valor}\")\n                \n                # 4. Actualizar el valor\n                secret_record.value = nuevo_valor\n                session.add(secret_record)\n                \n                print(\"Cambio aplicado en la sesi\u00f3n. Esperando commit...\")\n            else:\n                print(f\"ERROR: El secreto '{target_secret}' no existe en Mage Secrets.\")\n                return {\"status\": \"not_found\"}\n\n        # Al salir del bloque 'with', Mage hace el commit autom\u00e1tico\n        print(\"Commit realizado con \u00e9xito.\")\n\n        # 5. Verificaci\u00f3n final usando la funci\u00f3n est\u00e1ndar\n        verificacion = get_secret_value(target_secret)\n        print(f\"VERIFICACI\u00d3N FINAL: El nuevo valor recuperado es: {verificacion}\")\n        \n        return {\n            \"status\": \"success\",\n            \"valor_nuevo\": verificacion,\n            \"coincide\": verificacion == nuevo_valor\n        }\n\n    except Exception as e:\n        print(f\"FALLO CR\u00cdTICO: {str(e)}\")\n        # Importante para que el bloque se ponga en ROJO si falla\n        raise e", "file_path": "data_loaders/honorable_flower.py", "language": "python", "type": "data_loader", "uuid": "honorable_flower"}, "data_loaders/invoices_data_loader.py:data_loader:python:invoices data loader": {"content": "from mage_ai.data_preparation.shared.secrets import get_secret_value\nimport requests\nimport base64\nimport time\nimport pandas as pd\nfrom datetime import datetime, timedelta, timezone\nfrom dateutil import parser as date_parser\n\nCHUNK_DAYS = 1           # Tama\u00f1o del segmento (Requisito 7.1)\nPAGE_SIZE = 20           # Registros por petici\u00f3n (Requisito 7.2)\nMAX_RETRIES = 5          # Reintentos para Resiliencia\nINITIAL_BACKOFF = 5      # Segundos base para Backoff\nCOURTESY_WAIT = 0.5      # Pausa entre p\u00e1ginas (aumentado para QBO)\nCIRCUIT_BREAKER_THRESHOLD = 3  # Fallos consecutivos para activar circuit breaker\n\nQBO_URLS = {\n    'sandbox': \"https://sandbox-quickbooks.api.intuit.com/v3/company\",\n    'production': \"https://quickbooks.api.intuit.com/v3/company\"\n}\nTOKEN_URL = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\ndef get_new_access_token(client_id, client_secret, refresh_token, logger):\n    logger.info(f\"[AUTH] Iniciando autenticaci\u00f3n OAuth 2.0...\")\n    \n    auth_header = base64.b64encode(f\"{client_id}:{client_secret}\".encode()).decode()\n    headers = {\n        'Authorization': f'Basic {auth_header}',\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'Accept': 'application/json'\n    }\n    payload = {'grant_type': 'refresh_token', 'refresh_token': refresh_token}\n    \n    response = requests.post(TOKEN_URL, headers=headers, data=payload)\n    if response.status_code != 200:\n        logger.error(f\"[AUTH] Error en OAuth: {response.text}\")\n        raise Exception(f\"OAuth Failure: {response.status_code}\")\n    \n    token_data = response.json()\n    access_token = token_data.get('access_token')\n    new_refresh_token = token_data.get('refresh_token')\n    \n    logger.info(f\"[AUTH] Access Token obtenido exitosamente\")\n    \n    if new_refresh_token and new_refresh_token != refresh_token:\n        logger.warning(f\"[AUTH-ROTATION] NUEVO REFRESH TOKEN EMITIDO.\")\n        logger.warning(f\"[AUTH-ROTATION] Actualizar secret QBO_REFRESH_TOKEN!\")\n    else:\n        logger.info(f\"[AUTH] Refresh Token sin cambios.\")\n    \n    return access_token, new_refresh_token\n\n@data_loader\ndef load_data_from_quickbooks(*args, **kwargs):\n    logger = kwargs.get('logger')\n    \n    entity = 'Invoice'\n    start_date_str = kwargs.get('fecha_inicio')\n    end_date_str = kwargs.get('fecha_fin')\n    resume_from_str = kwargs.get('resume_from')\n        \n    if not start_date_str or not end_date_str:\n        raise ValueError(\"[VALIDATION] Error: 'fecha_inicio' y 'fecha_fin' son obligatorios.\")\n\n    def parse_to_utc(date_str):\n        dt = date_parser.parse(date_str)\n\n        if dt.tzinfo is None:\n            dt = dt.replace(tzinfo=timezone.utc)\n\n        return dt.astimezone(timezone.utc)\n    \n    dt_start = parse_to_utc(start_date_str)\n    dt_end = parse_to_utc(end_date_str)\n        \n    if dt_start >= dt_end:\n        raise ValueError(f\"[VALIDATION] Error: 'fecha_inicio' ({dt_start}) debe ser anterior a 'fecha_fin' ({dt_end}).\")\n    \n    if resume_from_str:\n        dt_resume = parse_to_utc(resume_from_str)\n        if dt_resume > dt_start and dt_resume < dt_end:\n            logger.info(f\"[RESUME] Reanudando desde checkpoint: {resume_from_str}\")\n            dt_start = dt_resume\n    \n    client_id = get_secret_value('QBO_CLIENT_ID')\n    client_secret = get_secret_value('QBO_CLIENT_SECRET')\n    refresh_token = get_secret_value('QBO_REFRESH_TOKEN')\n    realm_id = get_secret_value('QBO_REALM_ID')\n    qbo_environment = get_secret_value('QBO_ENVIRONMENT')\n    \n    qbo_base_url = QBO_URLS.get(qbo_environment.lower(), QBO_URLS['sandbox'])\n    logger.info(f\"[CONFIG] Entorno QBO: {qbo_environment} | URL Base: {qbo_base_url}\")\n    \n    # Variables de control\n    all_final_records = []\n    current_date = dt_start\n    current_refresh_token = refresh_token\n    consecutive_failures = 0\n    total_start_time = time.time()\n    last_successful_chunk_end = None\n    chunk_index = 0\n    pipeline_failed = False\n    original_fecha_fin = end_date_str\n\n    # Chunks de d\u00edas (Tramo)\n    dt_end_inclusive = dt_end + timedelta(seconds=1)\n    \n    while current_date < dt_end_inclusive:\n        chunk_index += 1\n        start_time_chunk = time.time()\n        next_date = min(current_date + timedelta(days=CHUNK_DAYS), dt_end_inclusive)\n        \n        chunk_start = current_date.strftime('%Y-%m-%dT%H:%M:%S+00:00')\n        chunk_end = next_date.strftime('%Y-%m-%dT%H:%M:%S+00:00')\n        \n        try:\n            access_token, new_refresh_token = get_new_access_token(\n                client_id, client_secret, current_refresh_token, logger\n            )\n\n            if new_refresh_token:\n                current_refresh_token = new_refresh_token\n            \n            logger.info(f\"[CHUNK] --- Iniciando Tramo: {chunk_start} a {chunk_end} ---\")\n            \n            start_position = 1\n            more_data_in_chunk = True\n            pages_in_chunk = 0\n            records_in_chunk = 0\n            \n            # Paginaci\u00f3n\n            while more_data_in_chunk:\n                query = (f\"SELECT * FROM {entity} \"\n                         f\"WHERE Metadata.LastUpdatedTime >= '{chunk_start}' \"\n                         f\"AND Metadata.LastUpdatedTime < '{chunk_end}' \"\n                         f\"STARTPOSITION {start_position} MAXRESULTS {PAGE_SIZE}\")\n                \n                url = f\"{qbo_base_url}/{realm_id}/query\"\n                headers = {'Authorization': f'Bearer {access_token}', 'Accept': 'application/json'}\n                \n                retries = 0\n                success = False\n                response = None\n                \n                while retries < MAX_RETRIES and not success:\n                    try:\n                        response = requests.get(url, headers=headers, params={'query': query})\n                        \n                        if response.status_code == 200:\n                            success = True\n                            consecutive_failures = 0\n                        elif response.status_code == 429:\n                            wait = (2 ** retries) * INITIAL_BACKOFF\n                            logger.warning(f\"[RATE-LIMIT] HTTP 429. Reintento {retries+1}/{MAX_RETRIES} en {wait}s\")\n                            time.sleep(wait)\n                            retries += 1\n                        elif response.status_code == 401:\n                            logger.warning(\"[AUTH] Token expirado, refrescando...\")\n                            access_token, new_refresh_token = get_new_access_token(\n                                client_id, client_secret, current_refresh_token, logger\n                            )\n                            if new_refresh_token:\n                                current_refresh_token = new_refresh_token\n                            headers = {'Authorization': f'Bearer {access_token}', 'Accept': 'application/json'}\n                        else:\n                            logger.error(f\"[API-ERROR] HTTP {response.status_code}: {response.text}\")\n                            retries += 1\n                            time.sleep(INITIAL_BACKOFF)\n                    except requests.exceptions.RequestException as e:\n                        logger.error(f\"[NETWORK-ERROR] {str(e)}. Reintento {retries+1}/{MAX_RETRIES}\")\n                        retries += 1\n                        time.sleep((2 ** retries) * INITIAL_BACKOFF)\n\n                if not success:\n                    consecutive_failures += 1\n                    logger.error(f\"[CHUNK-FAIL] Tramo {chunk_start} fall\u00f3 despu\u00e9s de {MAX_RETRIES} reintentos.\")\n                    \n                    # Circuit Breaker\n                    if consecutive_failures >= CIRCUIT_BREAKER_THRESHOLD:\n                        logger.critical(f\"[CIRCUIT-BREAKER] {consecutive_failures} fallos consecutivos. \"\n                                        f\"Pipeline detenido. \u00daltimo tramo exitoso: {last_successful_chunk_end}\")\n                        raise Exception(f\"Circuit Breaker activado tras {consecutive_failures} fallos consecutivos.\")\n                    break\n\n                # Metadatos\n                data_payload = response.json().get('QueryResponse', {}).get(entity, [])\n                \n                for record in data_payload:\n                    \n                    record_last_updated = record.get('MetaData', {}).get('LastUpdatedTime', '')\n                    \n                    all_final_records.append({\n                        'id': record.get('Id'),\n                        'payload': record,\n                        'ingested_at_utc': datetime.now(timezone.utc),\n                        'extract_window_start_utc': chunk_start,\n                        'extract_window_end_utc': chunk_end,\n                        'page_number': (start_position // PAGE_SIZE) + 1,\n                        'page_size': PAGE_SIZE,\n                        'request_payload': query,\n                        'source_last_updated_utc': record_last_updated\n                    })\n                \n                pages_in_chunk += 1\n                records_in_chunk += len(data_payload)\n                \n                if len(data_payload) < PAGE_SIZE:\n                    more_data_in_chunk = False\n                else:\n                    start_position += PAGE_SIZE\n                    time.sleep(COURTESY_WAIT)\n\n            # Metricas\n            duration_chunk = round(time.time() - start_time_chunk, 2)\n            \n            if records_in_chunk == 0:\n                logger.warning(f\"[VOLUMETRY] ALERTA: Tramo {chunk_start} a {chunk_end} retorn\u00f3 0 registros. \"\n                               f\"Verificar si es esperado o hay problema de filtros/datos.\")\n            \n            logger.info(f\"[METRICS] Tramo Finalizado: \"\n                        f\"Ventana: [{chunk_start} - {chunk_end}] | \"\n                        f\"P\u00e1ginas: {pages_in_chunk} | \"\n                        f\"Registros: {records_in_chunk} | \"\n                        f\"Duraci\u00f3n: {duration_chunk}s\")\n            \n            last_successful_chunk_end = chunk_end\n            \n        except Exception as e:\n            consecutive_failures += 1\n            pipeline_failed = True\n            logger.error(f\"[CHUNK-ERROR] Error en tramo #{chunk_index} ({chunk_start}): {str(e)}\")\n            \n            if last_successful_chunk_end:\n                logger.critical(f\"[CHECKPOINT] \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\")\n                logger.critical(f\"[CHECKPOINT] PIPELINE INTERRUMPIDO EN TRAMO #{chunk_index}\")\n                logger.critical(f\"[CHECKPOINT] \u00daltimo tramo exitoso: #{chunk_index - 1}\")\n                logger.critical(f\"[CHECKPOINT] \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\")\n                logger.critical(f\"[CHECKPOINT] PARA REANUDAR, usar par\u00e1metro:\")\n                logger.critical(f\"[CHECKPOINT] resume_from = '{last_successful_chunk_end}'\")\n                logger.critical(f\"[CHECKPOINT] \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\")\n            else:\n                logger.critical(f\"[CHECKPOINT] PIPELINE FALL\u00d3 EN EL PRIMER TRAMO.\")\n            \n            logger.warning(f\"[RECOVERY] Retornando {len(all_final_records)} registros de tramos exitosos anteriores.\")\n            break\n\n        current_date = next_date\n\n    # Resumen final\n    total_duration = round(time.time() - total_start_time, 2)\n    \n    if pipeline_failed:\n        logger.warning(f\"[EXTRACTION-PARTIAL] === EXTRACCI\u00d3N PARCIAL (CON ERRORES) ===\")\n        logger.warning(f\"[EXTRACTION-PARTIAL] Tramos completados exitosamente: {chunk_index - 1}\")\n    else:\n        logger.info(f\"[EXTRACTION-COMPLETE] === EXTRACCI\u00d3N FINALIZADA EXITOSAMENTE ===\")\n    \n    logger.info(f\"[EXTRACTION-COMPLETE] Total registros: {len(all_final_records)}\")\n    logger.info(f\"[EXTRACTION-COMPLETE] Duraci\u00f3n total: {total_duration}s\")\n    logger.info(f\"[EXTRACTION-COMPLETE] Entidad: {entity}\")\n    logger.info(f\"[EXTRACTION-COMPLETE] Rango solicitado: {start_date_str} a {end_date_str}\")\n    if resume_from_str:\n        logger.info(f\"[EXTRACTION-COMPLETE] Reanudado desde checkpoint: {resume_from_str}\")\n    \n    if len(all_final_records) == 0:\n        logger.warning(\"[VOLUMETRY] No se extrajeron registros. Verificar rango de fechas y datos en QBO.\")\n    \n    df = pd.DataFrame(all_final_records)\n    \n    if not df.empty:\n        df.attrs['last_checkpoint'] = last_successful_chunk_end\n        df.attrs['pipeline_failed'] = pipeline_failed\n        df.attrs['original_fecha_fin'] = original_fecha_fin\n    \n    return df", "file_path": "data_loaders/invoices_data_loader.py", "language": "python", "type": "data_loader", "uuid": "invoices_data_loader"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "transformers/transformer.py:transformer:python:transformer": {"content": "import pandas as pd\nfrom datetime import datetime\nimport time\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\n@transformer\ndef transform_raw_data(data, *args, **kwargs):\n    \"\"\"\n    A\u00f1ade metadatos obligatorios y realiza validaciones de integridad.\n    Cumple con los requisitos 7.3 (Capa RAW), 7.4 (Calidad) y 7.5 (M\u00e9tricas).\n    \"\"\"\n    # 1. Obtener el logger y par\u00e1metros (Requisito 7.5)\n    logger = kwargs.get('logger')\n    start_time_transform = time.time()\n    \n    logger.info(f\"--- INICIO DE FASE DE TRANSFORMACI\u00d3N: {datetime.utcnow().isoformat()} UTC ---\")\n    \n    start_date = kwargs.get('fecha_inicio')\n    end_date = kwargs.get('fecha_fin')\n    \n    if not data:\n        logger.warning(\"Fase de Transformaci\u00f3n: No se recibieron datos del Loader. Omitiendo proceso.\")\n        return pd.DataFrame()\n\n    transformed_rows = []\n    skipped_records = 0\n    duplicate_ids = set()\n    seen_ids = set()\n\n    # 2. Procesamiento y Validaci\u00f3n (Requisito 7.4)\n    logger.info(f\"Procesando {len(data)} registros recibidos...\")\n\n    for item in data:\n        record = item.get('raw_record')\n        record_id = record.get('Id')\n\n        # Validaci\u00f3n de integridad: Clave primaria no nula (Requisito 7.4)\n        if not record_id:\n            logger.error(\"Validaci\u00f3n de Integridad: Se encontr\u00f3 un registro sin ID. Omitiendo registro.\")\n            skipped_records += 1\n            continue\n\n        # Detecci\u00f3n de duplicados en el lote actual (Requisito 7.4)\n        if record_id in seen_ids:\n            duplicate_ids.add(record_id)\n            logger.warning(f\"Validaci\u00f3n de Integridad: ID {record_id} duplicado en el batch.\")\n        \n        seen_ids.add(record_id)\n\n        # 3. Construcci\u00f3n del objeto RAW con metadatos (Requisito 7.3)\n        transformed_rows.append({\n            'id': record_id,\n            'payload': record, # JSON/JSONB completo\n            'ingested_at_utc': datetime.utcnow(),\n            'extract_window_start_utc': start_date,\n            'extract_window_end_utc': end_date,\n            'page_number': item.get('meta_page'),\n            'page_size': item.get('meta_page_size'),\n            'request_payload': item.get('meta_query')\n        })\n\n    # 4. Resumen de M\u00e9tricas de Transformaci\u00f3n (Requisito 7.5)\n    duration = round(time.time() - start_time_transform, 4)\n    \n    logger.info(\"--- RESUMEN DE TRANSFORMACI\u00d3N Y CALIDAD ---\")\n    logger.info(f\"[METRIC] Registros procesados con \u00e9xito: {len(transformed_rows)}\")\n    logger.info(f\"[METRIC] Registros omitidos por error: {skipped_records}\")\n    logger.info(f\"[METRIC] IDs duplicados detectados: {len(duplicate_ids)}\")\n    logger.info(f\"[METRIC] Duraci\u00f3n de transformaci\u00f3n: {duration} segundos\")\n    \n    if skipped_records > 0:\n        logger.error(f\"Fase de Validaci\u00f3n finalizada con {skipped_records} errores de integridad.\")\n    else:\n        logger.info(\"Fase de Validaci\u00f3n y Calidad finalizada exitosamente.\")\n        \n    logger.info(\"--- FIN DE PROCESO TRANSFORMER ---\")\n\n    return pd.DataFrame(transformed_rows)", "file_path": "transformers/transformer.py", "language": "python", "type": "transformer", "uuid": "transformer"}, "pipelines/qb_customers_backfill/metadata.yaml:pipeline:yaml:qb customers backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - data_exporter\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: Data Loader\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: data_loader\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: Data Exporter\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - data_loader\n  uuid: data_exporter\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2026-02-01 04:42:03.827590+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_customers_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_customers_backfill\nvariables:\n  entity: Invoice\n  fecha_fin: '2026-01-05T13:16:17-08:00'\n  fecha_inicio: '2026-01-03T11:06:49-08:00'\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/qb_customers_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_customers_backfill/metadata"}, "pipelines/qb_customers_backfill/__init__.py:pipeline:python:qb customers backfill/  init  ": {"content": "", "file_path": "pipelines/qb_customers_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_customers_backfill/__init__"}, "pipelines/qb_invoices_backfill/metadata.yaml:pipeline:yaml:qb invoices backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - invoices_data_exporter\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: Invoices Data Loader\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: invoices_data_loader\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: Invoices Data Exporter\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - invoices_data_loader\n  uuid: invoices_data_exporter\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2026-02-01 04:42:03.827590+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_invoices_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_invoices_backfill\nvariables:\n  fecha_fin: '2026-01-05T13:16:17-08:00'\n  fecha_inicio: '2026-01-03T11:06:49-08:00'\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/qb_invoices_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_invoices_backfill/metadata"}, "pipelines/qb_invoices_backfill/__init__.py:pipeline:python:qb invoices backfill/  init  ": {"content": "", "file_path": "pipelines/qb_invoices_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_invoices_backfill/__init__"}, "pipelines/test/metadata.yaml:pipeline:yaml:test/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: honorable flower\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: honorable_flower\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2026-02-02 01:56:31.337634+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: test\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: test\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/test/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "test/metadata"}, "pipelines/test/__init__.py:pipeline:python:test/  init  ": {"content": "", "file_path": "pipelines/test/__init__.py", "language": "python", "type": "pipeline", "uuid": "test/__init__"}, "/home/src/default_repo/data_loaders/invoices_data_loader.py:data_loader:python:home/src/default repo/data loaders/invoices data loader": {"content": "from mage_ai.data_preparation.shared.secrets import get_secret_value\nimport requests\nimport base64\nimport time\nimport pandas as pd\nfrom datetime import datetime, timedelta, timezone\nfrom dateutil import parser as date_parser\n\nCHUNK_DAYS = 1           # Tama\u00f1o del segmento\nPAGE_SIZE = 10           # Registros por petici\u00f3n\nMAX_RETRIES = 5          # Reintentos\nINITIAL_BACKOFF = 5      # Segundos base para Backoff\nCOURTESY_WAIT = 0.5      # Pausa entre p\u00e1ginas\nCIRCUIT_BREAKER_THRESHOLD = 3  # Fallos consecutivos para activar circuit breaker\n\nQBO_URLS = {\n    'sandbox': \"https://sandbox-quickbooks.api.intuit.com/v3/company\",\n    'production': \"https://quickbooks.api.intuit.com/v3/company\"\n}\nTOKEN_URL = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\ndef get_new_access_token(client_id, client_secret, refresh_token, logger):\n    logger.info(f\"[AUTH] Iniciando autenticaci\u00f3n OAuth 2.0...\")\n    \n    auth_header = base64.b64encode(f\"{client_id}:{client_secret}\".encode()).decode()\n    headers = {\n        'Authorization': f'Basic {auth_header}',\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'Accept': 'application/json'\n    }\n    payload = {'grant_type': 'refresh_token', 'refresh_token': refresh_token}\n    \n    response = requests.post(TOKEN_URL, headers=headers, data=payload)\n    if response.status_code != 200:\n        logger.error(f\"[AUTH] Error en OAuth: {response.text}\")\n        raise Exception(f\"OAuth Failure: {response.status_code}\")\n    \n    token_data = response.json()\n    access_token = token_data.get('access_token')\n    new_refresh_token = token_data.get('refresh_token')\n    \n    logger.info(f\"[AUTH] Access Token obtenido exitosamente\")\n    \n    if new_refresh_token and new_refresh_token != refresh_token:\n        logger.warning(f\"[AUTH-ROTATION] NUEVO REFRESH TOKEN EMITIDO.\")\n        logger.warning(f\"[AUTH-ROTATION] Actualizar secreto QBO_REFRESH_TOKEN en Mage Secrets.\")\n        logger.info(f\"[AUTH-ROTATION] Token rotado, nuevo token: {new_refresh_token}.\")\n    else:\n        logger.info(f\"[AUTH] Refresh Token sin cambios.\")\n    \n    return access_token, new_refresh_token\n\n@data_loader\ndef load_data_from_quickbooks(*args, **kwargs):\n    logger = kwargs.get('logger')\n    \n    entity = 'Invoice'\n    logger.info(f\"[CONFIG] Entidad a extraer: {entity}\")\n    start_date_str = kwargs.get('fecha_inicio')\n    end_date_str = kwargs.get('fecha_fin')\n    resume_from_str = kwargs.get('resume_from')\n        \n    if not start_date_str or not end_date_str:\n        raise ValueError(\"[VALIDATION] Error: 'fecha_inicio' y 'fecha_fin' son obligatorios.\")\n\n    def parse_to_utc(date_str):\n        try:\n            dt = date_parser.parse(date_str)\n        except Exception as e:\n            raise ValueError(f\"[VALIDATION] Error parseando fecha '{date_str}': {str(e)}. \"\n                             f\"Formato esperado: ISO 8601 (ej: 2024-01-01T00:00:00Z)\")\n\n        if dt.tzinfo is None:\n            dt = dt.replace(tzinfo=timezone.utc)\n\n        return dt.astimezone(timezone.utc)\n    \n    dt_start = parse_to_utc(start_date_str)\n    dt_end = parse_to_utc(end_date_str)\n        \n    if dt_start >= dt_end:\n        raise ValueError(f\"[VALIDATION] Error: 'fecha_inicio' ({dt_start}) debe ser anterior a 'fecha_fin' ({dt_end}).\")\n    \n    if resume_from_str:\n        dt_resume = parse_to_utc(resume_from_str)\n        if dt_resume > dt_start and dt_resume < dt_end:\n            logger.info(f\"[RESUME] Reanudando desde checkpoint: {resume_from_str}\")\n            dt_start = dt_resume\n    \n    client_id = get_secret_value('QBO_CLIENT_ID')\n    if not client_id:\n        raise ValueError(\"[SECURITY] QBO_CLIENT_ID no configurado en Mage Secrets\")\n    client_secret = get_secret_value('QBO_CLIENT_SECRET')\n    if not client_secret:\n        raise ValueError(\"[SECURITY] QBO_CLIENT_SECRET no configurado en Mage Secrets\")\n    refresh_token = get_secret_value('QBO_REFRESH_TOKEN')\n    if not refresh_token:\n        raise ValueError(\"[SECURITY] QBO_REFRESH_TOKEN no configurado en Mage Secrets\")\n    realm_id = get_secret_value('QBO_REALM_ID')\n    if not realm_id:\n        raise ValueError(\"[SECURITY] QBO_REALM_ID no configurado en Mage Secrets\")\n    qbo_environment = get_secret_value('QBO_ENVIRONMENT')\n    if not qbo_environment:\n        raise ValueError(\"[SECURITY] QBO_ENVIRONMENT no configurado en Mage Secrets\")\n    \n    qbo_base_url = QBO_URLS.get(qbo_environment.lower(), QBO_URLS['sandbox'])\n    logger.info(f\"[CONFIG] Entorno QBO: {qbo_environment} | URL Base: {qbo_base_url}\")\n    \n    # Variables de control\n    all_final_records = []\n    current_date = dt_start\n    current_refresh_token = refresh_token\n    consecutive_failures = 0\n    total_start_time = time.time()\n    last_successful_chunk_end = None\n    chunk_index = 0\n    pipeline_failed = False\n    original_fecha_fin = end_date_str\n\n    # Chunks de d\u00edas (Tramo)\n    dt_end_inclusive = dt_end + timedelta(seconds=1)\n    \n    while current_date < dt_end_inclusive:\n        chunk_index += 1\n        start_time_chunk = time.time()\n        next_date = min(current_date + timedelta(days=CHUNK_DAYS), dt_end_inclusive)\n        \n        chunk_start = current_date.strftime('%Y-%m-%dT%H:%M:%S+00:00')\n        chunk_end = next_date.strftime('%Y-%m-%dT%H:%M:%S+00:00')\n        \n        try:\n            access_token, new_refresh_token = get_new_access_token(\n                client_id, client_secret, current_refresh_token, logger\n            )\n\n            if new_refresh_token:\n                current_refresh_token = new_refresh_token\n            \n            logger.info(f\"[CHUNK] --- Iniciando Tramo: {chunk_start} a {chunk_end} ---\")\n            \n            start_position = 1\n            more_data_in_chunk = True\n            pages_in_chunk = 0\n            records_in_chunk = 0\n            \n            # Paginaci\u00f3n\n            while more_data_in_chunk:\n                query = (f\"SELECT * FROM {entity} \"\n                         f\"WHERE Metadata.LastUpdatedTime >= '{chunk_start}' \"\n                         f\"AND Metadata.LastUpdatedTime < '{chunk_end}' \"\n                         f\"STARTPOSITION {start_position} MAXRESULTS {PAGE_SIZE}\")\n                \n                url = f\"{qbo_base_url}/{realm_id}/query\"\n                headers = {'Authorization': f'Bearer {access_token}', 'Accept': 'application/json'}\n                \n                retries = 0\n                success = False\n                response = None\n                \n                while retries < MAX_RETRIES and not success:\n                    try:\n                        response = requests.get(url, headers=headers, params={'query': query})\n                        \n                        if response.status_code == 200:\n                            success = True\n                            consecutive_failures = 0\n                        elif response.status_code == 429:\n                            wait = (2 ** retries) * INITIAL_BACKOFF\n                            logger.warning(f\"[RATE-LIMIT] HTTP 429. Reintento {retries+1}/{MAX_RETRIES} en {wait}s\")\n                            time.sleep(wait)\n                            retries += 1\n                        elif response.status_code == 401:\n                            logger.warning(\"[AUTH] Token expirado, refrescando...\")\n                            access_token, new_refresh_token = get_new_access_token(\n                                client_id, client_secret, current_refresh_token, logger\n                            )\n                            if new_refresh_token:\n                                current_refresh_token = new_refresh_token\n                            headers = {'Authorization': f'Bearer {access_token}', 'Accept': 'application/json'}\n                        else:\n                            logger.error(f\"[API-ERROR] HTTP {response.status_code}: {response.text}\")\n                            retries += 1\n                            time.sleep(INITIAL_BACKOFF)\n                    except requests.exceptions.RequestException as e:\n                        logger.error(f\"[NETWORK-ERROR] {str(e)}. Reintento {retries+1}/{MAX_RETRIES}\")\n                        retries += 1\n                        time.sleep((2 ** retries) * INITIAL_BACKOFF)\n\n                if not success:\n                    consecutive_failures += 1\n                    logger.error(f\"[CHUNK-FAIL] Tramo {chunk_start} fall\u00f3 despu\u00e9s de {MAX_RETRIES} reintentos.\")\n                    \n                    # Circuit Breaker\n                    if consecutive_failures >= CIRCUIT_BREAKER_THRESHOLD:\n                        logger.critical(f\"[CIRCUIT-BREAKER] {consecutive_failures} fallos consecutivos. \"\n                                        f\"Pipeline detenido. \u00daltimo tramo exitoso: {last_successful_chunk_end}\")\n                        raise Exception(f\"Circuit Breaker activado tras {consecutive_failures} fallos consecutivos.\")\n                    break\n\n                # Metadatos\n                data_payload = response.json().get('QueryResponse', {}).get(entity, [])\n                \n                for record in data_payload:\n                    \n                    record_last_updated = record.get('MetaData', {}).get('LastUpdatedTime', '')\n                    \n                    all_final_records.append({\n                        'id': record.get('Id'),\n                        'payload': record,\n                        'ingested_at_utc': datetime.now(timezone.utc),\n                        'extract_window_start_utc': chunk_start,\n                        'extract_window_end_utc': chunk_end,\n                        'page_number': (start_position // PAGE_SIZE) + 1,\n                        'page_size': PAGE_SIZE,\n                        'request_payload': query,\n                        'source_last_updated_utc': record_last_updated\n                    })\n                \n                pages_in_chunk += 1\n                records_in_chunk += len(data_payload)\n                \n                if len(data_payload) < PAGE_SIZE:\n                    more_data_in_chunk = False\n                else:\n                    start_position += PAGE_SIZE\n                    time.sleep(COURTESY_WAIT)\n\n            # Metricas\n            duration_chunk = round(time.time() - start_time_chunk, 2)\n            \n            if records_in_chunk == 0:\n                logger.warning(f\"[VOLUMETRY] ALERTA: Tramo {chunk_start} a {chunk_end} retorn\u00f3 0 registros. \"\n                               f\"Verificar si es esperado o hay problema de filtros/datos.\")\n            \n            logger.info(f\"[METRICS] Tramo Finalizado: \"\n                        f\"P\u00e1ginas: {pages_in_chunk} | \"\n                        f\"Registros: {records_in_chunk} | \"\n                        f\"Duraci\u00f3n: {duration_chunk}s\")\n            \n            last_successful_chunk_end = chunk_end\n            \n        except Exception as e:\n            consecutive_failures += 1\n            pipeline_failed = True\n            logger.error(f\"[CHUNK-ERROR] Error en tramo #{chunk_index} ({chunk_start}): {str(e)}\")\n            \n            if last_successful_chunk_end:\n                logger.critical(f\"[CHECKPOINT] \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\")\n                logger.critical(f\"[CHECKPOINT] PIPELINE INTERRUMPIDO EN TRAMO #{chunk_index}\")\n                logger.critical(f\"[CHECKPOINT] \u00daltimo tramo exitoso: #{chunk_index - 1}\")\n                logger.critical(f\"[CHECKPOINT] \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\")\n                logger.critical(f\"[CHECKPOINT] PARA REANUDAR, usar par\u00e1metro:\")\n                logger.critical(f\"[CHECKPOINT] resume_from = '{last_successful_chunk_end}'\")\n                logger.critical(f\"[CHECKPOINT] \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\")\n            else:\n                logger.critical(f\"[CHECKPOINT] PIPELINE FALL\u00d3 EN EL PRIMER TRAMO.\")\n            \n            logger.warning(f\"[RECOVERY] Retornando {len(all_final_records)} registros de tramos exitosos anteriores.\")\n            break\n\n        current_date = next_date\n\n    # Resumen final\n    total_duration = round(time.time() - total_start_time, 2)\n    \n    if pipeline_failed:\n        logger.warning(f\"[EXTRACTION-PARTIAL] === EXTRACCI\u00d3N PARCIAL (CON ERRORES) ===\")\n        logger.warning(f\"[EXTRACTION-PARTIAL] Tramos completados exitosamente: {chunk_index - 1}\")\n    else:\n        logger.info(f\"[EXTRACTION-COMPLETE] === EXTRACCI\u00d3N FINALIZADA EXITOSAMENTE ===\")\n    \n    logger.info(f\"[EXTRACTION-COMPLETE] Total registros: {len(all_final_records)}\")\n    logger.info(f\"[EXTRACTION-COMPLETE] Duraci\u00f3n total: {total_duration}s\")\n    logger.info(f\"[EXTRACTION-COMPLETE] Entidad: {entity}\")\n    logger.info(f\"[EXTRACTION-COMPLETE] Rango solicitado: {start_date_str} a {end_date_str}\")\n    if resume_from_str:\n        logger.info(f\"[EXTRACTION-COMPLETE] Reanudado desde checkpoint: {resume_from_str}\")\n    \n    if len(all_final_records) == 0:\n        logger.warning(\"[VOLUMETRY] No se extrajeron registros. Verificar rango de fechas y datos en QBO.\")\n    \n    df = pd.DataFrame(all_final_records)\n    \n    if not df.empty:\n        df.attrs['last_checkpoint'] = last_successful_chunk_end\n        df.attrs['pipeline_failed'] = pipeline_failed\n        df.attrs['original_fecha_fin'] = original_fecha_fin\n    \n    return df", "file_path": "/home/src/default_repo/data_loaders/invoices_data_loader.py", "language": "python", "type": "data_loader", "uuid": "invoices_data_loader"}, "/home/src/default_repo/data_exporters/invoices_data_exporter.py:data_exporter:python:home/src/default repo/data exporters/invoices data exporter": {"content": "from mage_ai.data_preparation.shared.secrets import get_secret_value\nimport psycopg2\nimport json\nimport time\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom dateutil import parser as date_parser\n\nMAX_DB_RETRIES = 3\nDB_RETRY_BACKOFF = 2\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\ndef get_db_connection_with_retry(db_params, logger):\n    retries = 0\n    while retries < MAX_DB_RETRIES:\n        try:\n            conn = psycopg2.connect(**db_params)\n            return conn\n        except psycopg2.OperationalError as e:\n            retries += 1\n            wait = (2 ** retries) * DB_RETRY_BACKOFF\n            logger.warning(f\"[DB-RETRY] Error de conexi\u00f3n: {str(e)}. Reintento {retries}/{MAX_DB_RETRIES} en {wait}s\")\n            time.sleep(wait)\n    raise Exception(f\"[DB-FAIL] No se pudo conectar a Postgres tras {MAX_DB_RETRIES} reintentos.\")\n\n\n@data_exporter\ndef export_data_to_postgres(df, *args, **kwargs):\n    logger = kwargs.get('logger')\n    start_time_load = time.time()\n    \n    entity = 'Invoice'\n    table_name = f\"qb_{entity.lower()}\"\n    schema_name = \"raw\"\n    \n    if df is None or df.empty:\n        logger.warning(f\"[VOLUMETRY] No hay datos para la entidad {table_name}. Fin de ejecuci\u00f3n.\")\n        return\n\n    try:\n        db_params = {\n            'host': get_secret_value('POSTGRES_HOST'),\n            'database': get_secret_value('POSTGRES_DB'),\n            'user': get_secret_value('POSTGRES_USER'),\n            'password': get_secret_value('POSTGRES_PASSWORD'),\n            'port': get_secret_value('POSTGRES_PORT')\n        }\n        conn = get_db_connection_with_retry(db_params, logger)\n        cur = conn.cursor()\n    except Exception as e:\n        logger.error(f\"[SECURITY/DB] Error al obtener secretos o conectar a Postgres: {str(e)}\")\n        raise e\n\n    try:\n        cur.execute(\"BEGIN;\")\n        cur.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema_name};\")\n        \n        create_table_query = f\"\"\"\n            CREATE TABLE IF NOT EXISTS {schema_name}.{table_name} (\n                id VARCHAR PRIMARY KEY,\n                payload JSONB,\n                ingested_at_utc TIMESTAMP WITH TIME ZONE,\n                extract_window_start_utc TIMESTAMP WITH TIME ZONE,\n                extract_window_end_utc TIMESTAMP WITH TIME ZONE,\n                page_number INT,\n                page_size INT,\n                request_payload TEXT,\n                source_last_updated_utc TIMESTAMP WITH TIME ZONE\n            );\n        \"\"\"\n        cur.execute(create_table_query)\n        conn.commit()\n        logger.info(f\"[DDL] Tabla {schema_name}.{table_name} creada/verificada exitosamente.\")\n    except Exception as e:\n        logger.error(f\"[DDL] Error creando infraestructura RAW: {str(e)}\")\n        conn.rollback()\n        raise e\n\n    try:\n        cur.execute(f\"SELECT COUNT(*) FROM {schema_name}.{table_name}\")\n        count_before = cur.fetchone()[0]\n    except:\n        count_before = 0\n\n    upsert_sql = f\"\"\"\n        INSERT INTO {schema_name}.{table_name} (\n            id, payload, ingested_at_utc, extract_window_start_utc, \n            extract_window_end_utc, page_number, page_size, request_payload,\n            source_last_updated_utc\n        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n        ON CONFLICT (id) DO UPDATE SET\n            payload = EXCLUDED.payload,\n            ingested_at_utc = EXCLUDED.ingested_at_utc,\n            extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n            extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n            page_number = EXCLUDED.page_number,\n            page_size = EXCLUDED.page_size,\n            request_payload = EXCLUDED.request_payload,\n            source_last_updated_utc = EXCLUDED.source_last_updated_utc;\n    \"\"\"\n\n    rows_processed = 0\n    rows_with_temporal_issues = 0\n    rows_skipped_null_id = 0\n    chunk_metrics = {}\n    \n    try:\n        for _, row in df.iterrows():\n            if not row['id'] or pd.isna(row['id']):\n                logger.error(f\"[VALIDATION] Registro con ID nulo omitido en exporter.\")\n                rows_skipped_null_id += 1\n                continue\n            \n            chunk_key = f\"{row['extract_window_start_utc']}|{row['extract_window_end_utc']}\"\n            if chunk_key not in chunk_metrics:\n                chunk_metrics[chunk_key] = {\n                    'count': 0,\n                    'window_start': row['extract_window_start_utc'],\n                    'window_end': row['extract_window_end_utc']\n                }\n            chunk_metrics[chunk_key]['count'] += 1\n            \n            ingested_at = row['ingested_at_utc']\n            window_end_str = row['extract_window_end_utc']\n            source_updated = row.get('source_last_updated_utc', '')\n            \n            try:\n                window_end_dt = date_parser.parse(window_end_str) if isinstance(window_end_str, str) else window_end_str\n                if ingested_at.tzinfo is None:\n                    ingested_at = ingested_at.replace(tzinfo=timezone.utc)\n                if window_end_dt.tzinfo is None:\n                    window_end_dt = window_end_dt.replace(tzinfo=timezone.utc)\n                    \n                if ingested_at < window_end_dt:\n                    rows_with_temporal_issues += 1\n                    logger.debug(f\"[TEMPORAL-WARNING] Registro {row['id']}: ingested_at < extract_window_end\")\n            except Exception as parse_error:\n                logger.debug(f\"[TEMPORAL-PARSE] No se pudo validar temporalidad para {row['id']}: {parse_error}\")\n            \n            source_updated_ts = None\n            if source_updated:\n                try:\n                    source_updated_ts = date_parser.parse(source_updated)\n                    window_start_str = row['extract_window_start_utc']\n                    window_start_dt = date_parser.parse(window_start_str) if isinstance(window_start_str, str) else window_start_str\n                    if source_updated_ts.tzinfo is None:\n                        source_updated_ts = source_updated_ts.replace(tzinfo=timezone.utc)\n                    if window_start_dt.tzinfo is None:\n                        window_start_dt = window_start_dt.replace(tzinfo=timezone.utc)\n                    if source_updated_ts < window_start_dt or source_updated_ts >= window_end_dt:\n                        logger.warning(f\"[TEMPORAL-ANOMALY] Registro {row['id']}: source_last_updated fuera de ventana\")\n                except:\n                    source_updated_ts = None\n            \n            retry_count = 0\n            while retry_count < MAX_DB_RETRIES:\n                try:\n                    cur.execute(upsert_sql, (\n                        str(row['id']), \n                        json.dumps(row['payload']), \n                        row['ingested_at_utc'],\n                        row['extract_window_start_utc'], \n                        row['extract_window_end_utc'],\n                        row['page_number'], \n                        row['page_size'], \n                        row['request_payload'],\n                        source_updated_ts\n                    ))\n                    rows_processed += 1\n                    break\n                except psycopg2.OperationalError as e:\n                    retry_count += 1\n                    if retry_count >= MAX_DB_RETRIES:\n                        raise e\n                    logger.warning(f\"[DB-RETRY] Error en INSERT, reintentando... {retry_count}/{MAX_DB_RETRIES}\")\n                    time.sleep(DB_RETRY_BACKOFF * retry_count)\n\n                    try:\n                        conn = get_db_connection_with_retry(db_params, logger)\n                        cur = conn.cursor()\n                    except:\n                        pass\n        \n        conn.commit()\n        logger.info(f\"[LOAD] Upsert exitoso: {rows_processed} filas procesadas en {table_name}.\")\n        \n    except Exception as e:\n        logger.error(f\"[LOAD] Fallo en la carga de datos: {str(e)}\")\n        conn.rollback()\n        raise e\n\n    # Metricas\n    try:\n        cur.execute(f\"SELECT COUNT(*) FROM {schema_name}.{table_name}\")\n        count_after = cur.fetchone()[0]\n        \n        new_inserts = count_after - count_before\n        updates = rows_processed - new_inserts if rows_processed > new_inserts else 0\n\n        if new_inserts < 0:\n            new_inserts = 0\n            updates = rows_processed\n        omitted = len(df) - rows_processed\n        \n        logger.info(\"--- REPORTE DE CALIDAD ---\")\n        logger.info(f\"[QUALITY] Entidad: {table_name}\")\n        logger.info(f\"[QUALITY] Registros en DataFrame: {len(df)}\")\n        logger.info(f\"[QUALITY] Total registros en tabla (antes): {count_before}\")\n        logger.info(f\"[QUALITY] Total registros en tabla (despu\u00e9s): {count_after}\")\n        \n        if rows_with_temporal_issues > 0:\n            logger.warning(f\"[TEMPORAL-QUALITY] {rows_with_temporal_issues} registros con posibles \"\n                           f\"inconsistencias temporales (ingested_at < extract_window_end).\")\n        \n        if rows_skipped_null_id > 0:\n            logger.warning(f\"[INTEGRITY] {rows_skipped_null_id} registros omitidos por ID nulo.\")\n        \n        logger.info(\"--- VOLUMETR\u00cdA POR TRAMO ---\")\n        for chunk_key, metrics in chunk_metrics.items():\n            logger.info(f\"[CHUNK-VOLUMETRY] Ventana: [{metrics['window_start']} - {metrics['window_end']}] | \"\n                        f\"Registros: {metrics['count']}\")\n\n            if metrics['count'] == 0:\n                logger.warning(f\"[VOLUMETRY] ALERTA: Tramo vac\u00edo detectado: {chunk_key}\")\n        logger.info(f\"[VOLUMETRY] Total tramos procesados: {len(chunk_metrics)}\")\n        \n        if rows_processed == 0 and not df.empty:\n            logger.warning(\"[QUALITY] ALERTA: El DataFrame ten\u00eda datos pero no se proces\u00f3 nada en Postgres.\")\n\n    except Exception as e:\n        logger.warning(f\"[QUALITY] No se pudo generar reporte de volumetr\u00eda: {str(e)}\")\n\n    try:\n        pipeline_failed = False\n        if hasattr(df, 'attrs'):\n            pipeline_failed = df.attrs.get('pipeline_failed', False)\n        \n        if pipeline_failed:\n            logger.warning(f\"[EXPORTER] Datos de extracci\u00f3n parcial exportados exitosamente.\")\n            logger.warning(f\"[EXPORTER] Revisar logs del Loader para instrucciones de reanudaci\u00f3n.\")\n        else:\n            logger.info(f\"[EXPORTER] Pipeline completado exitosamente.\")\n    except Exception as e:\n        logger.warning(f\"[EXPORTER] No se pudo verificar estado del pipeline: {str(e)}\")\n    \n    finally:\n        cur.close()\n        conn.close()\n\n    # Resumen final\n    duration = round(time.time() - start_time_load, 2)\n    logger.info(\"--- RESUMEN FINAL ---\")\n    logger.info(f\"Registros procesados: {rows_processed}\")\n    logger.info(f\"Nuevos: {new_inserts} | Actualizados: {updates} | Omitidos: {omitted}\")\n    logger.info(f\"Duraci\u00f3n: {duration} segundos\")\n    logger.info(f\"Coherencia Temporal: Marcas registradas en UTC\")\n    logger.info(\"--------------------------------------------\")", "file_path": "/home/src/default_repo/data_exporters/invoices_data_exporter.py", "language": "python", "type": "data_exporter", "uuid": "invoices_data_exporter"}, "/home/src/default_repo/data_loaders/customers_data_loader.py:data_loader:python:home/src/default repo/data loaders/customers data loader": {"content": "from mage_ai.data_preparation.shared.secrets import get_secret_value\nimport requests\nimport base64\nimport time\nimport pandas as pd\nfrom datetime import datetime, timedelta, timezone\nfrom dateutil import parser as date_parser\n\nCHUNK_DAYS = 1           # Tama\u00f1o del segmento\nPAGE_SIZE = 10           # Registros por petici\u00f3n\nMAX_RETRIES = 5          # Reintentos\nINITIAL_BACKOFF = 5      # Segundos base para Backoff\nCOURTESY_WAIT = 0.5      # Pausa entre p\u00e1ginas\nCIRCUIT_BREAKER_THRESHOLD = 3  # Fallos consecutivos para activar circuit breaker\n\nQBO_URLS = {\n    'sandbox': \"https://sandbox-quickbooks.api.intuit.com/v3/company\",\n    'production': \"https://quickbooks.api.intuit.com/v3/company\"\n}\nTOKEN_URL = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\ndef get_new_access_token(client_id, client_secret, refresh_token, logger):\n    logger.info(f\"[AUTH] Iniciando autenticaci\u00f3n OAuth 2.0...\")\n    \n    auth_header = base64.b64encode(f\"{client_id}:{client_secret}\".encode()).decode()\n    headers = {\n        'Authorization': f'Basic {auth_header}',\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'Accept': 'application/json'\n    }\n    payload = {'grant_type': 'refresh_token', 'refresh_token': refresh_token}\n    \n    response = requests.post(TOKEN_URL, headers=headers, data=payload)\n    if response.status_code != 200:\n        logger.error(f\"[AUTH] Error en OAuth: {response.text}\")\n        raise Exception(f\"OAuth Failure: {response.status_code}\")\n    \n    token_data = response.json()\n    access_token = token_data.get('access_token')\n    new_refresh_token = token_data.get('refresh_token')\n    \n    logger.info(f\"[AUTH] Access Token obtenido exitosamente\")\n    \n    if new_refresh_token and new_refresh_token != refresh_token:\n        logger.warning(f\"[AUTH-ROTATION] NUEVO REFRESH TOKEN EMITIDO.\")\n        logger.warning(f\"[AUTH-ROTATION] Actualizar secreto QBO_REFRESH_TOKEN en Mage Secrets.\")\n        logger.info(f\"[AUTH-ROTATION] Token rotado, nuevo token: {new_refresh_token}.\")\n    else:\n        logger.info(f\"[AUTH] Refresh Token sin cambios.\")\n    \n    return access_token, new_refresh_token\n\n@data_loader\ndef load_data_from_quickbooks(*args, **kwargs):\n    logger = kwargs.get('logger')\n    \n    entity = 'Customer'\n    logger.info(f\"[CONFIG] Entidad a extraer: {entity}\")\n    start_date_str = kwargs.get('fecha_inicio')\n    end_date_str = kwargs.get('fecha_fin')\n    resume_from_str = kwargs.get('resume_from')\n        \n    if not start_date_str or not end_date_str:\n        raise ValueError(\"[VALIDATION] Error: 'fecha_inicio' y 'fecha_fin' son obligatorios.\")\n\n    def parse_to_utc(date_str):\n        try:\n            dt = date_parser.parse(date_str)\n        except Exception as e:\n            raise ValueError(f\"[VALIDATION] Error parseando fecha '{date_str}': {str(e)}. \"\n                             f\"Formato esperado: ISO 8601 (ej: 2024-01-01T00:00:00Z)\")\n\n        if dt.tzinfo is None:\n            dt = dt.replace(tzinfo=timezone.utc)\n\n        return dt.astimezone(timezone.utc)\n    \n    dt_start = parse_to_utc(start_date_str)\n    dt_end = parse_to_utc(end_date_str)\n        \n    if dt_start >= dt_end:\n        raise ValueError(f\"[VALIDATION] Error: 'fecha_inicio' ({dt_start}) debe ser anterior a 'fecha_fin' ({dt_end}).\")\n    \n    if resume_from_str:\n        dt_resume = parse_to_utc(resume_from_str)\n        if dt_resume > dt_start and dt_resume < dt_end:\n            logger.info(f\"[RESUME] Reanudando desde checkpoint: {resume_from_str}\")\n            dt_start = dt_resume\n    \n    client_id = get_secret_value('QBO_CLIENT_ID')\n    if not client_id:\n        raise ValueError(\"[SECURITY] QBO_CLIENT_ID no configurado en Mage Secrets\")\n    client_secret = get_secret_value('QBO_CLIENT_SECRET')\n    if not client_secret:\n        raise ValueError(\"[SECURITY] QBO_CLIENT_SECRET no configurado en Mage Secrets\")\n    refresh_token = get_secret_value('QBO_REFRESH_TOKEN')\n    if not refresh_token:\n        raise ValueError(\"[SECURITY] QBO_REFRESH_TOKEN no configurado en Mage Secrets\")\n    realm_id = get_secret_value('QBO_REALM_ID')\n    if not realm_id:\n        raise ValueError(\"[SECURITY] QBO_REALM_ID no configurado en Mage Secrets\")\n    qbo_environment = get_secret_value('QBO_ENVIRONMENT')\n    if not qbo_environment:\n        raise ValueError(\"[SECURITY] QBO_ENVIRONMENT no configurado en Mage Secrets\")\n    \n    qbo_base_url = QBO_URLS.get(qbo_environment.lower(), QBO_URLS['sandbox'])\n    logger.info(f\"[CONFIG] Entorno QBO: {qbo_environment} | URL Base: {qbo_base_url}\")\n    \n    # Variables de control\n    all_final_records = []\n    current_date = dt_start\n    current_refresh_token = refresh_token\n    consecutive_failures = 0\n    total_start_time = time.time()\n    last_successful_chunk_end = None\n    chunk_index = 0\n    pipeline_failed = False\n    original_fecha_fin = end_date_str\n\n    # Chunks de d\u00edas (Tramo)\n    dt_end_inclusive = dt_end + timedelta(seconds=1)\n    \n    while current_date < dt_end_inclusive:\n        chunk_index += 1\n        start_time_chunk = time.time()\n        next_date = min(current_date + timedelta(days=CHUNK_DAYS), dt_end_inclusive)\n        \n        chunk_start = current_date.strftime('%Y-%m-%dT%H:%M:%S+00:00')\n        chunk_end = next_date.strftime('%Y-%m-%dT%H:%M:%S+00:00')\n        \n        try:\n            access_token, new_refresh_token = get_new_access_token(\n                client_id, client_secret, current_refresh_token, logger\n            )\n\n            if new_refresh_token:\n                current_refresh_token = new_refresh_token\n            \n            logger.info(f\"[CHUNK] --- Iniciando Tramo: {chunk_start} a {chunk_end} ---\")\n            \n            start_position = 1\n            more_data_in_chunk = True\n            pages_in_chunk = 0\n            records_in_chunk = 0\n            \n            # Paginaci\u00f3n\n            while more_data_in_chunk:\n                query = (f\"SELECT * FROM {entity} \"\n                         f\"WHERE Metadata.LastUpdatedTime >= '{chunk_start}' \"\n                         f\"AND Metadata.LastUpdatedTime < '{chunk_end}' \"\n                         f\"STARTPOSITION {start_position} MAXRESULTS {PAGE_SIZE}\")\n                \n                url = f\"{qbo_base_url}/{realm_id}/query\"\n                headers = {'Authorization': f'Bearer {access_token}', 'Accept': 'application/json'}\n                \n                retries = 0\n                success = False\n                response = None\n                \n                while retries < MAX_RETRIES and not success:\n                    try:\n                        response = requests.get(url, headers=headers, params={'query': query})\n                        \n                        if response.status_code == 200:\n                            success = True\n                            consecutive_failures = 0\n                        elif response.status_code == 429:\n                            wait = (2 ** retries) * INITIAL_BACKOFF\n                            logger.warning(f\"[RATE-LIMIT] HTTP 429. Reintento {retries+1}/{MAX_RETRIES} en {wait}s\")\n                            time.sleep(wait)\n                            retries += 1\n                        elif response.status_code == 401:\n                            logger.warning(\"[AUTH] Token expirado, refrescando...\")\n                            access_token, new_refresh_token = get_new_access_token(\n                                client_id, client_secret, current_refresh_token, logger\n                            )\n                            if new_refresh_token:\n                                current_refresh_token = new_refresh_token\n                            headers = {'Authorization': f'Bearer {access_token}', 'Accept': 'application/json'}\n                        else:\n                            logger.error(f\"[API-ERROR] HTTP {response.status_code}: {response.text}\")\n                            retries += 1\n                            time.sleep(INITIAL_BACKOFF)\n                    except requests.exceptions.RequestException as e:\n                        logger.error(f\"[NETWORK-ERROR] {str(e)}. Reintento {retries+1}/{MAX_RETRIES}\")\n                        retries += 1\n                        time.sleep((2 ** retries) * INITIAL_BACKOFF)\n\n                if not success:\n                    consecutive_failures += 1\n                    logger.error(f\"[CHUNK-FAIL] Tramo {chunk_start} fall\u00f3 despu\u00e9s de {MAX_RETRIES} reintentos.\")\n                    \n                    # Circuit Breaker\n                    if consecutive_failures >= CIRCUIT_BREAKER_THRESHOLD:\n                        logger.critical(f\"[CIRCUIT-BREAKER] {consecutive_failures} fallos consecutivos. \"\n                                        f\"Pipeline detenido. \u00daltimo tramo exitoso: {last_successful_chunk_end}\")\n                        raise Exception(f\"Circuit Breaker activado tras {consecutive_failures} fallos consecutivos.\")\n                    break\n\n                # Metadatos\n                data_payload = response.json().get('QueryResponse', {}).get(entity, [])\n                \n                for record in data_payload:\n                    \n                    record_last_updated = record.get('MetaData', {}).get('LastUpdatedTime', '')\n                    \n                    all_final_records.append({\n                        'id': record.get('Id'),\n                        'payload': record,\n                        'ingested_at_utc': datetime.now(timezone.utc),\n                        'extract_window_start_utc': chunk_start,\n                        'extract_window_end_utc': chunk_end,\n                        'page_number': (start_position // PAGE_SIZE) + 1,\n                        'page_size': PAGE_SIZE,\n                        'request_payload': query,\n                        'source_last_updated_utc': record_last_updated\n                    })\n                \n                pages_in_chunk += 1\n                records_in_chunk += len(data_payload)\n                \n                if len(data_payload) < PAGE_SIZE:\n                    more_data_in_chunk = False\n                else:\n                    start_position += PAGE_SIZE\n                    time.sleep(COURTESY_WAIT)\n\n            # Metricas\n            duration_chunk = round(time.time() - start_time_chunk, 2)\n            \n            if records_in_chunk == 0:\n                logger.warning(f\"[VOLUMETRY] ALERTA: Tramo {chunk_start} a {chunk_end} retorn\u00f3 0 registros. \"\n                               f\"Verificar si es esperado o hay problema de filtros/datos.\")\n            \n            logger.info(f\"[METRICS] Tramo Finalizado: \"\n                        f\"P\u00e1ginas: {pages_in_chunk} | \"\n                        f\"Registros: {records_in_chunk} | \"\n                        f\"Duraci\u00f3n: {duration_chunk}s\")\n            \n            last_successful_chunk_end = chunk_end\n            \n        except Exception as e:\n            consecutive_failures += 1\n            pipeline_failed = True\n            logger.error(f\"[CHUNK-ERROR] Error en tramo #{chunk_index} ({chunk_start}): {str(e)}\")\n            \n            if last_successful_chunk_end:\n                logger.critical(f\"[CHECKPOINT] \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\")\n                logger.critical(f\"[CHECKPOINT] PIPELINE INTERRUMPIDO EN TRAMO #{chunk_index}\")\n                logger.critical(f\"[CHECKPOINT] \u00daltimo tramo exitoso: #{chunk_index - 1}\")\n                logger.critical(f\"[CHECKPOINT] \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\")\n                logger.critical(f\"[CHECKPOINT] PARA REANUDAR, usar par\u00e1metro:\")\n                logger.critical(f\"[CHECKPOINT] resume_from = '{last_successful_chunk_end}'\")\n                logger.critical(f\"[CHECKPOINT] \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\")\n            else:\n                logger.critical(f\"[CHECKPOINT] PIPELINE FALL\u00d3 EN EL PRIMER TRAMO.\")\n            \n            logger.warning(f\"[RECOVERY] Retornando {len(all_final_records)} registros de tramos exitosos anteriores.\")\n            break\n\n        current_date = next_date\n\n    # Resumen final\n    total_duration = round(time.time() - total_start_time, 2)\n    \n    if pipeline_failed:\n        logger.warning(f\"[EXTRACTION-PARTIAL] === EXTRACCI\u00d3N PARCIAL (CON ERRORES) ===\")\n        logger.warning(f\"[EXTRACTION-PARTIAL] Tramos completados exitosamente: {chunk_index - 1}\")\n    else:\n        logger.info(f\"[EXTRACTION-COMPLETE] === EXTRACCI\u00d3N FINALIZADA EXITOSAMENTE ===\")\n    \n    logger.info(f\"[EXTRACTION-COMPLETE] Total registros: {len(all_final_records)}\")\n    logger.info(f\"[EXTRACTION-COMPLETE] Duraci\u00f3n total: {total_duration}s\")\n    logger.info(f\"[EXTRACTION-COMPLETE] Entidad: {entity}\")\n    logger.info(f\"[EXTRACTION-COMPLETE] Rango solicitado: {start_date_str} a {end_date_str}\")\n    if resume_from_str:\n        logger.info(f\"[EXTRACTION-COMPLETE] Reanudado desde checkpoint: {resume_from_str}\")\n    \n    if len(all_final_records) == 0:\n        logger.warning(\"[VOLUMETRY] No se extrajeron registros. Verificar rango de fechas y datos en QBO.\")\n    \n    df = pd.DataFrame(all_final_records)\n    \n    if not df.empty:\n        df.attrs['last_checkpoint'] = last_successful_chunk_end\n        df.attrs['pipeline_failed'] = pipeline_failed\n        df.attrs['original_fecha_fin'] = original_fecha_fin\n    \n    return df", "file_path": "/home/src/default_repo/data_loaders/customers_data_loader.py", "language": "python", "type": "data_loader", "uuid": "customers_data_loader"}, "/home/src/default_repo/data_exporters/customers_data_exporter.py:data_exporter:python:home/src/default repo/data exporters/customers data exporter": {"content": "from mage_ai.data_preparation.shared.secrets import get_secret_value\nimport psycopg2\nimport json\nimport time\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom dateutil import parser as date_parser\n\nMAX_DB_RETRIES = 3\nDB_RETRY_BACKOFF = 2\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\ndef get_db_connection_with_retry(db_params, logger):\n    retries = 0\n    while retries < MAX_DB_RETRIES:\n        try:\n            conn = psycopg2.connect(**db_params)\n            return conn\n        except psycopg2.OperationalError as e:\n            retries += 1\n            wait = (2 ** retries) * DB_RETRY_BACKOFF\n            logger.warning(f\"[DB-RETRY] Error de conexi\u00f3n: {str(e)}. Reintento {retries}/{MAX_DB_RETRIES} en {wait}s\")\n            time.sleep(wait)\n    raise Exception(f\"[DB-FAIL] No se pudo conectar a Postgres tras {MAX_DB_RETRIES} reintentos.\")\n\n\n@data_exporter\ndef export_data_to_postgres(df, *args, **kwargs):\n    logger = kwargs.get('logger')\n    start_time_load = time.time()\n    \n    entity = 'Customer'\n    table_name = f\"qb_{entity.lower()}\"\n    schema_name = \"raw\"\n    \n    if df is None or df.empty:\n        logger.warning(f\"[VOLUMETRY] No hay datos para la entidad {table_name}. Fin de ejecuci\u00f3n.\")\n        return\n\n    try:\n        db_params = {\n            'host': get_secret_value('POSTGRES_HOST'),\n            'database': get_secret_value('POSTGRES_DB'),\n            'user': get_secret_value('POSTGRES_USER'),\n            'password': get_secret_value('POSTGRES_PASSWORD'),\n            'port': get_secret_value('POSTGRES_PORT')\n        }\n        conn = get_db_connection_with_retry(db_params, logger)\n        cur = conn.cursor()\n    except Exception as e:\n        logger.error(f\"[SECURITY/DB] Error al obtener secretos o conectar a Postgres: {str(e)}\")\n        raise e\n\n    try:\n        cur.execute(\"BEGIN;\")\n        cur.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema_name};\")\n        \n        create_table_query = f\"\"\"\n            CREATE TABLE IF NOT EXISTS {schema_name}.{table_name} (\n                id VARCHAR PRIMARY KEY,\n                payload JSONB,\n                ingested_at_utc TIMESTAMP WITH TIME ZONE,\n                extract_window_start_utc TIMESTAMP WITH TIME ZONE,\n                extract_window_end_utc TIMESTAMP WITH TIME ZONE,\n                page_number INT,\n                page_size INT,\n                request_payload TEXT,\n                source_last_updated_utc TIMESTAMP WITH TIME ZONE\n            );\n        \"\"\"\n        cur.execute(create_table_query)\n        conn.commit()\n        logger.info(f\"[DDL] Tabla {schema_name}.{table_name} creada/verificada exitosamente.\")\n    except Exception as e:\n        logger.error(f\"[DDL] Error creando infraestructura RAW: {str(e)}\")\n        conn.rollback()\n        raise e\n\n    try:\n        cur.execute(f\"SELECT COUNT(*) FROM {schema_name}.{table_name}\")\n        count_before = cur.fetchone()[0]\n    except:\n        count_before = 0\n\n    upsert_sql = f\"\"\"\n        INSERT INTO {schema_name}.{table_name} (\n            id, payload, ingested_at_utc, extract_window_start_utc, \n            extract_window_end_utc, page_number, page_size, request_payload,\n            source_last_updated_utc\n        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n        ON CONFLICT (id) DO UPDATE SET\n            payload = EXCLUDED.payload,\n            ingested_at_utc = EXCLUDED.ingested_at_utc,\n            extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n            extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n            page_number = EXCLUDED.page_number,\n            page_size = EXCLUDED.page_size,\n            request_payload = EXCLUDED.request_payload,\n            source_last_updated_utc = EXCLUDED.source_last_updated_utc;\n    \"\"\"\n\n    rows_processed = 0\n    rows_with_temporal_issues = 0\n    rows_skipped_null_id = 0\n    chunk_metrics = {}\n    \n    try:\n        for _, row in df.iterrows():\n            if not row['id'] or pd.isna(row['id']):\n                logger.error(f\"[VALIDATION] Registro con ID nulo omitido en exporter.\")\n                rows_skipped_null_id += 1\n                continue\n            \n            chunk_key = f\"{row['extract_window_start_utc']}|{row['extract_window_end_utc']}\"\n            if chunk_key not in chunk_metrics:\n                chunk_metrics[chunk_key] = {\n                    'count': 0,\n                    'window_start': row['extract_window_start_utc'],\n                    'window_end': row['extract_window_end_utc']\n                }\n            chunk_metrics[chunk_key]['count'] += 1\n            \n            ingested_at = row['ingested_at_utc']\n            window_end_str = row['extract_window_end_utc']\n            source_updated = row.get('source_last_updated_utc', '')\n            \n            try:\n                window_end_dt = date_parser.parse(window_end_str) if isinstance(window_end_str, str) else window_end_str\n                if ingested_at.tzinfo is None:\n                    ingested_at = ingested_at.replace(tzinfo=timezone.utc)\n                if window_end_dt.tzinfo is None:\n                    window_end_dt = window_end_dt.replace(tzinfo=timezone.utc)\n                    \n                if ingested_at < window_end_dt:\n                    rows_with_temporal_issues += 1\n                    logger.debug(f\"[TEMPORAL-WARNING] Registro {row['id']}: ingested_at < extract_window_end\")\n            except Exception as parse_error:\n                logger.debug(f\"[TEMPORAL-PARSE] No se pudo validar temporalidad para {row['id']}: {parse_error}\")\n            \n            source_updated_ts = None\n            if source_updated:\n                try:\n                    source_updated_ts = date_parser.parse(source_updated)\n                    window_start_str = row['extract_window_start_utc']\n                    window_start_dt = date_parser.parse(window_start_str) if isinstance(window_start_str, str) else window_start_str\n                    if source_updated_ts.tzinfo is None:\n                        source_updated_ts = source_updated_ts.replace(tzinfo=timezone.utc)\n                    if window_start_dt.tzinfo is None:\n                        window_start_dt = window_start_dt.replace(tzinfo=timezone.utc)\n                    if source_updated_ts < window_start_dt or source_updated_ts >= window_end_dt:\n                        logger.warning(f\"[TEMPORAL-ANOMALY] Registro {row['id']}: source_last_updated fuera de ventana\")\n                except:\n                    source_updated_ts = None\n            \n            retry_count = 0\n            while retry_count < MAX_DB_RETRIES:\n                try:\n                    cur.execute(upsert_sql, (\n                        str(row['id']), \n                        json.dumps(row['payload']), \n                        row['ingested_at_utc'],\n                        row['extract_window_start_utc'], \n                        row['extract_window_end_utc'],\n                        row['page_number'], \n                        row['page_size'], \n                        row['request_payload'],\n                        source_updated_ts\n                    ))\n                    rows_processed += 1\n                    break\n                except psycopg2.OperationalError as e:\n                    retry_count += 1\n                    if retry_count >= MAX_DB_RETRIES:\n                        raise e\n                    logger.warning(f\"[DB-RETRY] Error en INSERT, reintentando... {retry_count}/{MAX_DB_RETRIES}\")\n                    time.sleep(DB_RETRY_BACKOFF * retry_count)\n\n                    try:\n                        conn = get_db_connection_with_retry(db_params, logger)\n                        cur = conn.cursor()\n                    except:\n                        pass\n        \n        conn.commit()\n        logger.info(f\"[LOAD] Upsert exitoso: {rows_processed} filas procesadas en {table_name}.\")\n        \n    except Exception as e:\n        logger.error(f\"[LOAD] Fallo en la carga de datos: {str(e)}\")\n        conn.rollback()\n        raise e\n\n    # Metricas\n    try:\n        cur.execute(f\"SELECT COUNT(*) FROM {schema_name}.{table_name}\")\n        count_after = cur.fetchone()[0]\n        \n        new_inserts = count_after - count_before\n        updates = rows_processed - new_inserts if rows_processed > new_inserts else 0\n\n        if new_inserts < 0:\n            new_inserts = 0\n            updates = rows_processed\n        omitted = len(df) - rows_processed\n        \n        logger.info(\"--- REPORTE DE CALIDAD ---\")\n        logger.info(f\"[QUALITY] Entidad: {table_name}\")\n        logger.info(f\"[QUALITY] Registros en DataFrame: {len(df)}\")\n        logger.info(f\"[QUALITY] Total registros en tabla (antes): {count_before}\")\n        logger.info(f\"[QUALITY] Total registros en tabla (despu\u00e9s): {count_after}\")\n        \n        if rows_with_temporal_issues > 0:\n            logger.warning(f\"[TEMPORAL-QUALITY] {rows_with_temporal_issues} registros con posibles \"\n                           f\"inconsistencias temporales (ingested_at < extract_window_end).\")\n        \n        if rows_skipped_null_id > 0:\n            logger.warning(f\"[INTEGRITY] {rows_skipped_null_id} registros omitidos por ID nulo.\")\n        \n        logger.info(\"--- VOLUMETR\u00cdA POR TRAMO ---\")\n        for chunk_key, metrics in chunk_metrics.items():\n            logger.info(f\"[CHUNK-VOLUMETRY] Ventana: [{metrics['window_start']} - {metrics['window_end']}] | \"\n                        f\"Registros: {metrics['count']}\")\n\n            if metrics['count'] == 0:\n                logger.warning(f\"[VOLUMETRY] ALERTA: Tramo vac\u00edo detectado: {chunk_key}\")\n        logger.info(f\"[VOLUMETRY] Total tramos procesados: {len(chunk_metrics)}\")\n        \n        if rows_processed == 0 and not df.empty:\n            logger.warning(\"[QUALITY] ALERTA: El DataFrame ten\u00eda datos pero no se proces\u00f3 nada en Postgres.\")\n\n    except Exception as e:\n        logger.warning(f\"[QUALITY] No se pudo generar reporte de volumetr\u00eda: {str(e)}\")\n\n    try:\n        pipeline_failed = False\n        if hasattr(df, 'attrs'):\n            pipeline_failed = df.attrs.get('pipeline_failed', False)\n        \n        if pipeline_failed:\n            logger.warning(f\"[EXPORTER] Datos de extracci\u00f3n parcial exportados exitosamente.\")\n            logger.warning(f\"[EXPORTER] Revisar logs del Loader para instrucciones de reanudaci\u00f3n.\")\n        else:\n            logger.info(f\"[EXPORTER] Pipeline completado exitosamente.\")\n    except Exception as e:\n        logger.warning(f\"[EXPORTER] No se pudo verificar estado del pipeline: {str(e)}\")\n    \n    finally:\n        cur.close()\n        conn.close()\n\n    # Resumen final\n    duration = round(time.time() - start_time_load, 2)\n    logger.info(\"--- RESUMEN FINAL ---\")\n    logger.info(f\"Registros procesados: {rows_processed}\")\n    logger.info(f\"Nuevos: {new_inserts} | Actualizados: {updates} | Omitidos: {omitted}\")\n    logger.info(f\"Duraci\u00f3n: {duration} segundos\")\n    logger.info(f\"Coherencia Temporal: Marcas registradas en UTC\")\n    logger.info(\"--------------------------------------------\")", "file_path": "/home/src/default_repo/data_exporters/customers_data_exporter.py", "language": "python", "type": "data_exporter", "uuid": "customers_data_exporter"}, "/home/src/default_repo/data_loaders/items_data_loader.py:data_loader:python:home/src/default repo/data loaders/items data loader": {"content": "from mage_ai.data_preparation.shared.secrets import get_secret_value\nimport requests\nimport base64\nimport time\nimport pandas as pd\nfrom datetime import datetime, timedelta, timezone\nfrom dateutil import parser as date_parser\n\nCHUNK_DAYS = 1           # Tama\u00f1o del segmento\nPAGE_SIZE = 10           # Registros por petici\u00f3n\nMAX_RETRIES = 5          # Reintentos\nINITIAL_BACKOFF = 5      # Segundos base para Backoff\nCOURTESY_WAIT = 0.5      # Pausa entre p\u00e1ginas\nCIRCUIT_BREAKER_THRESHOLD = 3  # Fallos consecutivos para activar circuit breaker\n\nQBO_URLS = {\n    'sandbox': \"https://sandbox-quickbooks.api.intuit.com/v3/company\",\n    'production': \"https://quickbooks.api.intuit.com/v3/company\"\n}\nTOKEN_URL = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\ndef get_new_access_token(client_id, client_secret, refresh_token, logger):\n    logger.info(f\"[AUTH] Iniciando autenticaci\u00f3n OAuth 2.0...\")\n    \n    auth_header = base64.b64encode(f\"{client_id}:{client_secret}\".encode()).decode()\n    headers = {\n        'Authorization': f'Basic {auth_header}',\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'Accept': 'application/json'\n    }\n    payload = {'grant_type': 'refresh_token', 'refresh_token': refresh_token}\n    \n    response = requests.post(TOKEN_URL, headers=headers, data=payload)\n    if response.status_code != 200:\n        logger.error(f\"[AUTH] Error en OAuth: {response.text}\")\n        raise Exception(f\"OAuth Failure: {response.status_code}\")\n    \n    token_data = response.json()\n    access_token = token_data.get('access_token')\n    new_refresh_token = token_data.get('refresh_token')\n    \n    logger.info(f\"[AUTH] Access Token obtenido exitosamente\")\n    \n    if new_refresh_token and new_refresh_token != refresh_token:\n        logger.warning(f\"[AUTH-ROTATION] NUEVO REFRESH TOKEN EMITIDO.\")\n        logger.warning(f\"[AUTH-ROTATION] Actualizar secreto QBO_REFRESH_TOKEN en Mage Secrets.\")\n        logger.info(f\"[AUTH-ROTATION] Token rotado, nuevo token: {new_refresh_token}.\")\n    else:\n        logger.info(f\"[AUTH] Refresh Token sin cambios.\")\n    \n    return access_token, new_refresh_token\n\n@data_loader\ndef load_data_from_quickbooks(*args, **kwargs):\n    logger = kwargs.get('logger')\n    \n    entity = 'Item'\n    logger.info(f\"[CONFIG] Entidad a extraer: {entity}\")\n    start_date_str = kwargs.get('fecha_inicio')\n    end_date_str = kwargs.get('fecha_fin')\n    resume_from_str = kwargs.get('resume_from')\n        \n    if not start_date_str or not end_date_str:\n        raise ValueError(\"[VALIDATION] Error: 'fecha_inicio' y 'fecha_fin' son obligatorios.\")\n\n    def parse_to_utc(date_str):\n        try:\n            dt = date_parser.parse(date_str)\n        except Exception as e:\n            raise ValueError(f\"[VALIDATION] Error parseando fecha '{date_str}': {str(e)}. \"\n                             f\"Formato esperado: ISO 8601 (ej: 2024-01-01T00:00:00Z)\")\n\n        if dt.tzinfo is None:\n            dt = dt.replace(tzinfo=timezone.utc)\n\n        return dt.astimezone(timezone.utc)\n    \n    dt_start = parse_to_utc(start_date_str)\n    dt_end = parse_to_utc(end_date_str)\n        \n    if dt_start >= dt_end:\n        raise ValueError(f\"[VALIDATION] Error: 'fecha_inicio' ({dt_start}) debe ser anterior a 'fecha_fin' ({dt_end}).\")\n    \n    if resume_from_str:\n        dt_resume = parse_to_utc(resume_from_str)\n        if dt_resume > dt_start and dt_resume < dt_end:\n            logger.info(f\"[RESUME] Reanudando desde checkpoint: {resume_from_str}\")\n            dt_start = dt_resume\n    \n    client_id = get_secret_value('QBO_CLIENT_ID')\n    if not client_id:\n        raise ValueError(\"[SECURITY] QBO_CLIENT_ID no configurado en Mage Secrets\")\n    client_secret = get_secret_value('QBO_CLIENT_SECRET')\n    if not client_secret:\n        raise ValueError(\"[SECURITY] QBO_CLIENT_SECRET no configurado en Mage Secrets\")\n    refresh_token = get_secret_value('QBO_REFRESH_TOKEN')\n    if not refresh_token:\n        raise ValueError(\"[SECURITY] QBO_REFRESH_TOKEN no configurado en Mage Secrets\")\n    realm_id = get_secret_value('QBO_REALM_ID')\n    if not realm_id:\n        raise ValueError(\"[SECURITY] QBO_REALM_ID no configurado en Mage Secrets\")\n    qbo_environment = get_secret_value('QBO_ENVIRONMENT')\n    if not qbo_environment:\n        raise ValueError(\"[SECURITY] QBO_ENVIRONMENT no configurado en Mage Secrets\")\n    \n    qbo_base_url = QBO_URLS.get(qbo_environment.lower(), QBO_URLS['sandbox'])\n    logger.info(f\"[CONFIG] Entorno QBO: {qbo_environment} | URL Base: {qbo_base_url}\")\n    \n    # Variables de control\n    all_final_records = []\n    current_date = dt_start\n    current_refresh_token = refresh_token\n    consecutive_failures = 0\n    total_start_time = time.time()\n    last_successful_chunk_end = None\n    chunk_index = 0\n    pipeline_failed = False\n    original_fecha_fin = end_date_str\n\n    # Chunks de d\u00edas (Tramo)\n    dt_end_inclusive = dt_end + timedelta(seconds=1)\n    \n    while current_date < dt_end_inclusive:\n        chunk_index += 1\n        start_time_chunk = time.time()\n        next_date = min(current_date + timedelta(days=CHUNK_DAYS), dt_end_inclusive)\n        \n        chunk_start = current_date.strftime('%Y-%m-%dT%H:%M:%S+00:00')\n        chunk_end = next_date.strftime('%Y-%m-%dT%H:%M:%S+00:00')\n        \n        try:\n            access_token, new_refresh_token = get_new_access_token(\n                client_id, client_secret, current_refresh_token, logger\n            )\n\n            if new_refresh_token:\n                current_refresh_token = new_refresh_token\n            \n            logger.info(f\"[CHUNK] --- Iniciando Tramo: {chunk_start} a {chunk_end} ---\")\n            \n            start_position = 1\n            more_data_in_chunk = True\n            pages_in_chunk = 0\n            records_in_chunk = 0\n            \n            # Paginaci\u00f3n\n            while more_data_in_chunk:\n                query = (f\"SELECT * FROM {entity} \"\n                         f\"WHERE Metadata.LastUpdatedTime >= '{chunk_start}' \"\n                         f\"AND Metadata.LastUpdatedTime < '{chunk_end}' \"\n                         f\"STARTPOSITION {start_position} MAXRESULTS {PAGE_SIZE}\")\n                \n                url = f\"{qbo_base_url}/{realm_id}/query\"\n                headers = {'Authorization': f'Bearer {access_token}', 'Accept': 'application/json'}\n                \n                retries = 0\n                success = False\n                response = None\n                \n                while retries < MAX_RETRIES and not success:\n                    try:\n                        response = requests.get(url, headers=headers, params={'query': query})\n                        \n                        if response.status_code == 200:\n                            success = True\n                            consecutive_failures = 0\n                        elif response.status_code == 429:\n                            wait = (2 ** retries) * INITIAL_BACKOFF\n                            logger.warning(f\"[RATE-LIMIT] HTTP 429. Reintento {retries+1}/{MAX_RETRIES} en {wait}s\")\n                            time.sleep(wait)\n                            retries += 1\n                        elif response.status_code == 401:\n                            logger.warning(\"[AUTH] Token expirado, refrescando...\")\n                            access_token, new_refresh_token = get_new_access_token(\n                                client_id, client_secret, current_refresh_token, logger\n                            )\n                            if new_refresh_token:\n                                current_refresh_token = new_refresh_token\n                            headers = {'Authorization': f'Bearer {access_token}', 'Accept': 'application/json'}\n                        else:\n                            logger.error(f\"[API-ERROR] HTTP {response.status_code}: {response.text}\")\n                            retries += 1\n                            time.sleep(INITIAL_BACKOFF)\n                    except requests.exceptions.RequestException as e:\n                        logger.error(f\"[NETWORK-ERROR] {str(e)}. Reintento {retries+1}/{MAX_RETRIES}\")\n                        retries += 1\n                        time.sleep((2 ** retries) * INITIAL_BACKOFF)\n\n                if not success:\n                    consecutive_failures += 1\n                    logger.error(f\"[CHUNK-FAIL] Tramo {chunk_start} fall\u00f3 despu\u00e9s de {MAX_RETRIES} reintentos.\")\n                    \n                    # Circuit Breaker\n                    if consecutive_failures >= CIRCUIT_BREAKER_THRESHOLD:\n                        logger.critical(f\"[CIRCUIT-BREAKER] {consecutive_failures} fallos consecutivos. \"\n                                        f\"Pipeline detenido. \u00daltimo tramo exitoso: {last_successful_chunk_end}\")\n                        raise Exception(f\"Circuit Breaker activado tras {consecutive_failures} fallos consecutivos.\")\n                    break\n\n                # Metadatos\n                data_payload = response.json().get('QueryResponse', {}).get(entity, [])\n                \n                for record in data_payload:\n                    \n                    record_last_updated = record.get('MetaData', {}).get('LastUpdatedTime', '')\n                    \n                    all_final_records.append({\n                        'id': record.get('Id'),\n                        'payload': record,\n                        'ingested_at_utc': datetime.now(timezone.utc),\n                        'extract_window_start_utc': chunk_start,\n                        'extract_window_end_utc': chunk_end,\n                        'page_number': (start_position // PAGE_SIZE) + 1,\n                        'page_size': PAGE_SIZE,\n                        'request_payload': query,\n                        'source_last_updated_utc': record_last_updated\n                    })\n                \n                pages_in_chunk += 1\n                records_in_chunk += len(data_payload)\n                \n                if len(data_payload) < PAGE_SIZE:\n                    more_data_in_chunk = False\n                else:\n                    start_position += PAGE_SIZE\n                    time.sleep(COURTESY_WAIT)\n\n            # Metricas\n            duration_chunk = round(time.time() - start_time_chunk, 2)\n            \n            if records_in_chunk == 0:\n                logger.warning(f\"[VOLUMETRY] ALERTA: Tramo {chunk_start} a {chunk_end} retorn\u00f3 0 registros. \"\n                               f\"Verificar si es esperado o hay problema de filtros/datos.\")\n            \n            logger.info(f\"[METRICS] Tramo Finalizado: \"\n                        f\"P\u00e1ginas: {pages_in_chunk} | \"\n                        f\"Registros: {records_in_chunk} | \"\n                        f\"Duraci\u00f3n: {duration_chunk}s\")\n            \n            last_successful_chunk_end = chunk_end\n            \n        except Exception as e:\n            consecutive_failures += 1\n            pipeline_failed = True\n            logger.error(f\"[CHUNK-ERROR] Error en tramo #{chunk_index} ({chunk_start}): {str(e)}\")\n            \n            if last_successful_chunk_end:\n                logger.critical(f\"[CHECKPOINT] \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\")\n                logger.critical(f\"[CHECKPOINT] PIPELINE INTERRUMPIDO EN TRAMO #{chunk_index}\")\n                logger.critical(f\"[CHECKPOINT] \u00daltimo tramo exitoso: #{chunk_index - 1}\")\n                logger.critical(f\"[CHECKPOINT] \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\")\n                logger.critical(f\"[CHECKPOINT] PARA REANUDAR, usar par\u00e1metro:\")\n                logger.critical(f\"[CHECKPOINT] resume_from = '{last_successful_chunk_end}'\")\n                logger.critical(f\"[CHECKPOINT] \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\")\n            else:\n                logger.critical(f\"[CHECKPOINT] PIPELINE FALL\u00d3 EN EL PRIMER TRAMO.\")\n            \n            logger.warning(f\"[RECOVERY] Retornando {len(all_final_records)} registros de tramos exitosos anteriores.\")\n            break\n\n        current_date = next_date\n\n    # Resumen final\n    total_duration = round(time.time() - total_start_time, 2)\n    \n    if pipeline_failed:\n        logger.warning(f\"[EXTRACTION-PARTIAL] === EXTRACCI\u00d3N PARCIAL (CON ERRORES) ===\")\n        logger.warning(f\"[EXTRACTION-PARTIAL] Tramos completados exitosamente: {chunk_index - 1}\")\n    else:\n        logger.info(f\"[EXTRACTION-COMPLETE] === EXTRACCI\u00d3N FINALIZADA EXITOSAMENTE ===\")\n    \n    logger.info(f\"[EXTRACTION-COMPLETE] Total registros: {len(all_final_records)}\")\n    logger.info(f\"[EXTRACTION-COMPLETE] Duraci\u00f3n total: {total_duration}s\")\n    logger.info(f\"[EXTRACTION-COMPLETE] Entidad: {entity}\")\n    logger.info(f\"[EXTRACTION-COMPLETE] Rango solicitado: {start_date_str} a {end_date_str}\")\n    if resume_from_str:\n        logger.info(f\"[EXTRACTION-COMPLETE] Reanudado desde checkpoint: {resume_from_str}\")\n    \n    if len(all_final_records) == 0:\n        logger.warning(\"[VOLUMETRY] No se extrajeron registros. Verificar rango de fechas y datos en QBO.\")\n    \n    df = pd.DataFrame(all_final_records)\n    \n    if not df.empty:\n        df.attrs['last_checkpoint'] = last_successful_chunk_end\n        df.attrs['pipeline_failed'] = pipeline_failed\n        df.attrs['original_fecha_fin'] = original_fecha_fin\n    \n    return df", "file_path": "/home/src/default_repo/data_loaders/items_data_loader.py", "language": "python", "type": "data_loader", "uuid": "items_data_loader"}, "/home/src/default_repo/data_exporters/items_data_exporter.py:data_exporter:python:home/src/default repo/data exporters/items data exporter": {"content": "from mage_ai.data_preparation.shared.secrets import get_secret_value\nimport psycopg2\nimport json\nimport time\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom dateutil import parser as date_parser\n\nMAX_DB_RETRIES = 3\nDB_RETRY_BACKOFF = 2\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\ndef get_db_connection_with_retry(db_params, logger):\n    retries = 0\n    while retries < MAX_DB_RETRIES:\n        try:\n            conn = psycopg2.connect(**db_params)\n            return conn\n        except psycopg2.OperationalError as e:\n            retries += 1\n            wait = (2 ** retries) * DB_RETRY_BACKOFF\n            logger.warning(f\"[DB-RETRY] Error de conexi\u00f3n: {str(e)}. Reintento {retries}/{MAX_DB_RETRIES} en {wait}s\")\n            time.sleep(wait)\n    raise Exception(f\"[DB-FAIL] No se pudo conectar a Postgres tras {MAX_DB_RETRIES} reintentos.\")\n\n\n@data_exporter\ndef export_data_to_postgres(df, *args, **kwargs):\n    logger = kwargs.get('logger')\n    start_time_load = time.time()\n    \n    entity = 'Item'\n    table_name = f\"qb_{entity.lower()}\"\n    schema_name = \"raw\"\n    \n    if df is None or df.empty:\n        logger.warning(f\"[VOLUMETRY] No hay datos para la entidad {table_name}. Fin de ejecuci\u00f3n.\")\n        return\n\n    try:\n        db_params = {\n            'host': get_secret_value('POSTGRES_HOST'),\n            'database': get_secret_value('POSTGRES_DB'),\n            'user': get_secret_value('POSTGRES_USER'),\n            'password': get_secret_value('POSTGRES_PASSWORD'),\n            'port': get_secret_value('POSTGRES_PORT')\n        }\n        conn = get_db_connection_with_retry(db_params, logger)\n        cur = conn.cursor()\n    except Exception as e:\n        logger.error(f\"[SECURITY/DB] Error al obtener secretos o conectar a Postgres: {str(e)}\")\n        raise e\n\n    try:\n        cur.execute(\"BEGIN;\")\n        cur.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema_name};\")\n        \n        create_table_query = f\"\"\"\n            CREATE TABLE IF NOT EXISTS {schema_name}.{table_name} (\n                id VARCHAR PRIMARY KEY,\n                payload JSONB,\n                ingested_at_utc TIMESTAMP WITH TIME ZONE,\n                extract_window_start_utc TIMESTAMP WITH TIME ZONE,\n                extract_window_end_utc TIMESTAMP WITH TIME ZONE,\n                page_number INT,\n                page_size INT,\n                request_payload TEXT,\n                source_last_updated_utc TIMESTAMP WITH TIME ZONE\n            );\n        \"\"\"\n        cur.execute(create_table_query)\n        conn.commit()\n        logger.info(f\"[DDL] Tabla {schema_name}.{table_name} creada/verificada exitosamente.\")\n    except Exception as e:\n        logger.error(f\"[DDL] Error creando infraestructura RAW: {str(e)}\")\n        conn.rollback()\n        raise e\n\n    try:\n        cur.execute(f\"SELECT COUNT(*) FROM {schema_name}.{table_name}\")\n        count_before = cur.fetchone()[0]\n    except:\n        count_before = 0\n\n    upsert_sql = f\"\"\"\n        INSERT INTO {schema_name}.{table_name} (\n            id, payload, ingested_at_utc, extract_window_start_utc, \n            extract_window_end_utc, page_number, page_size, request_payload,\n            source_last_updated_utc\n        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n        ON CONFLICT (id) DO UPDATE SET\n            payload = EXCLUDED.payload,\n            ingested_at_utc = EXCLUDED.ingested_at_utc,\n            extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n            extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n            page_number = EXCLUDED.page_number,\n            page_size = EXCLUDED.page_size,\n            request_payload = EXCLUDED.request_payload,\n            source_last_updated_utc = EXCLUDED.source_last_updated_utc;\n    \"\"\"\n\n    rows_processed = 0\n    rows_with_temporal_issues = 0\n    rows_skipped_null_id = 0\n    chunk_metrics = {}\n    \n    try:\n        for _, row in df.iterrows():\n            if not row['id'] or pd.isna(row['id']):\n                logger.error(f\"[VALIDATION] Registro con ID nulo omitido en exporter.\")\n                rows_skipped_null_id += 1\n                continue\n            \n            chunk_key = f\"{row['extract_window_start_utc']}|{row['extract_window_end_utc']}\"\n            if chunk_key not in chunk_metrics:\n                chunk_metrics[chunk_key] = {\n                    'count': 0,\n                    'window_start': row['extract_window_start_utc'],\n                    'window_end': row['extract_window_end_utc']\n                }\n            chunk_metrics[chunk_key]['count'] += 1\n            \n            ingested_at = row['ingested_at_utc']\n            window_end_str = row['extract_window_end_utc']\n            source_updated = row.get('source_last_updated_utc', '')\n            \n            try:\n                window_end_dt = date_parser.parse(window_end_str) if isinstance(window_end_str, str) else window_end_str\n                if ingested_at.tzinfo is None:\n                    ingested_at = ingested_at.replace(tzinfo=timezone.utc)\n                if window_end_dt.tzinfo is None:\n                    window_end_dt = window_end_dt.replace(tzinfo=timezone.utc)\n                    \n                if ingested_at < window_end_dt:\n                    rows_with_temporal_issues += 1\n                    logger.debug(f\"[TEMPORAL-WARNING] Registro {row['id']}: ingested_at < extract_window_end\")\n            except Exception as parse_error:\n                logger.debug(f\"[TEMPORAL-PARSE] No se pudo validar temporalidad para {row['id']}: {parse_error}\")\n            \n            source_updated_ts = None\n            if source_updated:\n                try:\n                    source_updated_ts = date_parser.parse(source_updated)\n                    window_start_str = row['extract_window_start_utc']\n                    window_start_dt = date_parser.parse(window_start_str) if isinstance(window_start_str, str) else window_start_str\n                    if source_updated_ts.tzinfo is None:\n                        source_updated_ts = source_updated_ts.replace(tzinfo=timezone.utc)\n                    if window_start_dt.tzinfo is None:\n                        window_start_dt = window_start_dt.replace(tzinfo=timezone.utc)\n                    if source_updated_ts < window_start_dt or source_updated_ts >= window_end_dt:\n                        logger.warning(f\"[TEMPORAL-ANOMALY] Registro {row['id']}: source_last_updated fuera de ventana\")\n                except:\n                    source_updated_ts = None\n            \n            retry_count = 0\n            while retry_count < MAX_DB_RETRIES:\n                try:\n                    cur.execute(upsert_sql, (\n                        str(row['id']), \n                        json.dumps(row['payload']), \n                        row['ingested_at_utc'],\n                        row['extract_window_start_utc'], \n                        row['extract_window_end_utc'],\n                        row['page_number'], \n                        row['page_size'], \n                        row['request_payload'],\n                        source_updated_ts\n                    ))\n                    rows_processed += 1\n                    break\n                except psycopg2.OperationalError as e:\n                    retry_count += 1\n                    if retry_count >= MAX_DB_RETRIES:\n                        raise e\n                    logger.warning(f\"[DB-RETRY] Error en INSERT, reintentando... {retry_count}/{MAX_DB_RETRIES}\")\n                    time.sleep(DB_RETRY_BACKOFF * retry_count)\n\n                    try:\n                        conn = get_db_connection_with_retry(db_params, logger)\n                        cur = conn.cursor()\n                    except:\n                        pass\n        \n        conn.commit()\n        logger.info(f\"[LOAD] Upsert exitoso: {rows_processed} filas procesadas en {table_name}.\")\n        \n    except Exception as e:\n        logger.error(f\"[LOAD] Fallo en la carga de datos: {str(e)}\")\n        conn.rollback()\n        raise e\n\n    # Metricas\n    try:\n        cur.execute(f\"SELECT COUNT(*) FROM {schema_name}.{table_name}\")\n        count_after = cur.fetchone()[0]\n        \n        new_inserts = count_after - count_before\n        updates = rows_processed - new_inserts if rows_processed > new_inserts else 0\n\n        if new_inserts < 0:\n            new_inserts = 0\n            updates = rows_processed\n        omitted = len(df) - rows_processed\n        \n        logger.info(\"--- REPORTE DE CALIDAD ---\")\n        logger.info(f\"[QUALITY] Entidad: {table_name}\")\n        logger.info(f\"[QUALITY] Registros en DataFrame: {len(df)}\")\n        logger.info(f\"[QUALITY] Total registros en tabla (antes): {count_before}\")\n        logger.info(f\"[QUALITY] Total registros en tabla (despu\u00e9s): {count_after}\")\n        \n        if rows_with_temporal_issues > 0:\n            logger.warning(f\"[TEMPORAL-QUALITY] {rows_with_temporal_issues} registros con posibles \"\n                           f\"inconsistencias temporales (ingested_at < extract_window_end).\")\n        \n        if rows_skipped_null_id > 0:\n            logger.warning(f\"[INTEGRITY] {rows_skipped_null_id} registros omitidos por ID nulo.\")\n        \n        logger.info(\"--- VOLUMETR\u00cdA POR TRAMO ---\")\n        for chunk_key, metrics in chunk_metrics.items():\n            logger.info(f\"[CHUNK-VOLUMETRY] Ventana: [{metrics['window_start']} - {metrics['window_end']}] | \"\n                        f\"Registros: {metrics['count']}\")\n\n            if metrics['count'] == 0:\n                logger.warning(f\"[VOLUMETRY] ALERTA: Tramo vac\u00edo detectado: {chunk_key}\")\n        logger.info(f\"[VOLUMETRY] Total tramos procesados: {len(chunk_metrics)}\")\n        \n        if rows_processed == 0 and not df.empty:\n            logger.warning(\"[QUALITY] ALERTA: El DataFrame ten\u00eda datos pero no se proces\u00f3 nada en Postgres.\")\n\n    except Exception as e:\n        logger.warning(f\"[QUALITY] No se pudo generar reporte de volumetr\u00eda: {str(e)}\")\n\n    try:\n        pipeline_failed = False\n        if hasattr(df, 'attrs'):\n            pipeline_failed = df.attrs.get('pipeline_failed', False)\n        \n        if pipeline_failed:\n            logger.warning(f\"[EXPORTER] Datos de extracci\u00f3n parcial exportados exitosamente.\")\n            logger.warning(f\"[EXPORTER] Revisar logs del Loader para instrucciones de reanudaci\u00f3n.\")\n        else:\n            logger.info(f\"[EXPORTER] Pipeline completado exitosamente.\")\n    except Exception as e:\n        logger.warning(f\"[EXPORTER] No se pudo verificar estado del pipeline: {str(e)}\")\n    \n    finally:\n        cur.close()\n        conn.close()\n\n    # Resumen final\n    duration = round(time.time() - start_time_load, 2)\n    logger.info(\"--- RESUMEN FINAL ---\")\n    logger.info(f\"Registros procesados: {rows_processed}\")\n    logger.info(f\"Nuevos: {new_inserts} | Actualizados: {updates} | Omitidos: {omitted}\")\n    logger.info(f\"Duraci\u00f3n: {duration} segundos\")\n    logger.info(f\"Coherencia Temporal: Marcas registradas en UTC\")\n    logger.info(\"--------------------------------------------\")", "file_path": "/home/src/default_repo/data_exporters/items_data_exporter.py", "language": "python", "type": "data_exporter", "uuid": "items_data_exporter"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}