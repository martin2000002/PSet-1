{"block_file": {"data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "pipelines/example_pipeline/metadata.yaml:pipeline:yaml:example pipeline/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - fill_in_missing_values\n  name: load_titanic\n  status: not_executed\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_titanic\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - export_titanic_clean\n  name: fill_in_missing_values\n  status: not_executed\n  type: transformer\n  upstream_blocks:\n  - load_titanic\n  uuid: fill_in_missing_values\n- all_upstream_blocks_executed: true\n  downstream_blocks: []\n  name: export_titanic_clean\n  status: not_executed\n  type: data_exporter\n  upstream_blocks:\n  - fill_in_missing_values\n  uuid: export_titanic_clean\nname: example_pipeline\ntype: python\nuuid: example_pipeline\nwidgets: []\n", "file_path": "pipelines/example_pipeline/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "example_pipeline/metadata"}, "pipelines/example_pipeline/__init__.py:pipeline:python:example pipeline/  init  ": {"content": "", "file_path": "pipelines/example_pipeline/__init__.py", "language": "python", "type": "pipeline", "uuid": "example_pipeline/__init__"}, "/home/src/default_repo/data_loaders/invoices_data_loader.py:data_loader:python:home/src/default repo/data loaders/invoices data loader": {"content": "from mage_ai.data_preparation.shared.secrets import get_secret_value\nimport requests\nimport base64\nimport time\nimport pandas as pd\nfrom datetime import datetime, timedelta, timezone\nfrom dateutil import parser as date_parser\n\nCHUNK_DAYS = 1           # Tama\u00f1o del segmento (Requisito 7.1)\nPAGE_SIZE = 20           # Registros por petici\u00f3n (Requisito 7.2)\nMAX_RETRIES = 5          # Reintentos para Resiliencia\nINITIAL_BACKOFF = 5      # Segundos base para Backoff\nCOURTESY_WAIT = 0.5      # Pausa entre p\u00e1ginas (aumentado para QBO)\nCIRCUIT_BREAKER_THRESHOLD = 3  # Fallos consecutivos para activar circuit breaker\n\nQBO_URLS = {\n    'sandbox': \"https://sandbox-quickbooks.api.intuit.com/v3/company\",\n    'production': \"https://quickbooks.api.intuit.com/v3/company\"\n}\nTOKEN_URL = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\ndef get_new_access_token(client_id, client_secret, refresh_token, logger):\n    logger.info(f\"[AUTH] Iniciando autenticaci\u00f3n OAuth 2.0...\")\n    \n    auth_header = base64.b64encode(f\"{client_id}:{client_secret}\".encode()).decode()\n    headers = {\n        'Authorization': f'Basic {auth_header}',\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'Accept': 'application/json'\n    }\n    payload = {'grant_type': 'refresh_token', 'refresh_token': refresh_token}\n    \n    response = requests.post(TOKEN_URL, headers=headers, data=payload)\n    if response.status_code != 200:\n        logger.error(f\"[AUTH] Error en OAuth: {response.text}\")\n        raise Exception(f\"OAuth Failure: {response.status_code}\")\n    \n    token_data = response.json()\n    access_token = token_data.get('access_token')\n    new_refresh_token = token_data.get('refresh_token')\n    \n    logger.info(f\"[AUTH] Access Token obtenido exitosamente\")\n    \n    if new_refresh_token and new_refresh_token != refresh_token:\n        logger.warning(f\"[AUTH-ROTATION] NUEVO REFRESH TOKEN EMITIDO.\")\n        logger.warning(f\"[AUTH-ROTATION] Actualizar secret QBO_REFRESH_TOKEN!\")\n    else:\n        logger.info(f\"[AUTH] Refresh Token sin cambios.\")\n    \n    return access_token, new_refresh_token\n\n@data_loader\ndef load_data_from_quickbooks(*args, **kwargs):\n    logger = kwargs.get('logger')\n    \n    entity = 'Invoice'\n    start_date_str = kwargs.get('fecha_inicio')\n    end_date_str = kwargs.get('fecha_fin')\n    resume_from_str = kwargs.get('resume_from')\n        \n    if not start_date_str or not end_date_str:\n        raise ValueError(\"[VALIDATION] Error: 'fecha_inicio' y 'fecha_fin' son obligatorios.\")\n\n    def parse_to_utc(date_str):\n        dt = date_parser.parse(date_str)\n\n        if dt.tzinfo is None:\n            dt = dt.replace(tzinfo=timezone.utc)\n\n        return dt.astimezone(timezone.utc)\n    \n    dt_start = parse_to_utc(start_date_str)\n    dt_end = parse_to_utc(end_date_str)\n        \n    if dt_start >= dt_end:\n        raise ValueError(f\"[VALIDATION] Error: 'fecha_inicio' ({dt_start}) debe ser anterior a 'fecha_fin' ({dt_end}).\")\n    \n    if resume_from_str:\n        dt_resume = parse_to_utc(resume_from_str)\n        if dt_resume > dt_start and dt_resume < dt_end:\n            logger.info(f\"[RESUME] Reanudando desde checkpoint: {resume_from_str}\")\n            dt_start = dt_resume\n    \n    client_id = get_secret_value('QBO_CLIENT_ID')\n    client_secret = get_secret_value('QBO_CLIENT_SECRET')\n    refresh_token = get_secret_value('QBO_REFRESH_TOKEN')\n    realm_id = get_secret_value('QBO_REALM_ID')\n    qbo_environment = get_secret_value('QBO_ENVIRONMENT')\n    \n    qbo_base_url = QBO_URLS.get(qbo_environment.lower(), QBO_URLS['sandbox'])\n    logger.info(f\"[CONFIG] Entorno QBO: {qbo_environment} | URL Base: {qbo_base_url}\")\n    \n    # Variables de control\n    all_final_records = []\n    current_date = dt_start\n    current_refresh_token = refresh_token\n    consecutive_failures = 0\n    total_start_time = time.time()\n    last_successful_chunk_end = None\n    chunk_index = 0\n    pipeline_failed = False\n    original_fecha_fin = end_date_str\n\n    # Chunks de d\u00edas (Tramo)\n    dt_end_inclusive = dt_end + timedelta(seconds=1)\n    \n    while current_date < dt_end_inclusive:\n        chunk_index += 1\n        start_time_chunk = time.time()\n        next_date = min(current_date + timedelta(days=CHUNK_DAYS), dt_end_inclusive)\n        \n        chunk_start = current_date.strftime('%Y-%m-%dT%H:%M:%S+00:00')\n        chunk_end = next_date.strftime('%Y-%m-%dT%H:%M:%S+00:00')\n        \n        try:\n            access_token, new_refresh_token = get_new_access_token(\n                client_id, client_secret, current_refresh_token, logger\n            )\n\n            if new_refresh_token:\n                current_refresh_token = new_refresh_token\n            \n            logger.info(f\"[CHUNK] --- Iniciando Tramo: {chunk_start} a {chunk_end} ---\")\n            \n            start_position = 1\n            more_data_in_chunk = True\n            pages_in_chunk = 0\n            records_in_chunk = 0\n            \n            # Paginaci\u00f3n\n            while more_data_in_chunk:\n                query = (f\"SELECT * FROM {entity} \"\n                         f\"WHERE Metadata.LastUpdatedTime >= '{chunk_start}' \"\n                         f\"AND Metadata.LastUpdatedTime < '{chunk_end}' \"\n                         f\"STARTPOSITION {start_position} MAXRESULTS {PAGE_SIZE}\")\n                \n                url = f\"{qbo_base_url}/{realm_id}/query\"\n                headers = {'Authorization': f'Bearer {access_token}', 'Accept': 'application/json'}\n                \n                retries = 0\n                success = False\n                response = None\n                \n                while retries < MAX_RETRIES and not success:\n                    try:\n                        response = requests.get(url, headers=headers, params={'query': query})\n                        \n                        if response.status_code == 200:\n                            success = True\n                            consecutive_failures = 0\n                        elif response.status_code == 429:\n                            wait = (2 ** retries) * INITIAL_BACKOFF\n                            logger.warning(f\"[RATE-LIMIT] HTTP 429. Reintento {retries+1}/{MAX_RETRIES} en {wait}s\")\n                            time.sleep(wait)\n                            retries += 1\n                        elif response.status_code == 401:\n                            logger.warning(\"[AUTH] Token expirado, refrescando...\")\n                            access_token, new_refresh_token = get_new_access_token(\n                                client_id, client_secret, current_refresh_token, logger\n                            )\n                            if new_refresh_token:\n                                current_refresh_token = new_refresh_token\n                            headers = {'Authorization': f'Bearer {access_token}', 'Accept': 'application/json'}\n                        else:\n                            logger.error(f\"[API-ERROR] HTTP {response.status_code}: {response.text}\")\n                            retries += 1\n                            time.sleep(INITIAL_BACKOFF)\n                    except requests.exceptions.RequestException as e:\n                        logger.error(f\"[NETWORK-ERROR] {str(e)}. Reintento {retries+1}/{MAX_RETRIES}\")\n                        retries += 1\n                        time.sleep((2 ** retries) * INITIAL_BACKOFF)\n\n                if not success:\n                    consecutive_failures += 1\n                    logger.error(f\"[CHUNK-FAIL] Tramo {chunk_start} fall\u00f3 despu\u00e9s de {MAX_RETRIES} reintentos.\")\n                    \n                    # Circuit Breaker\n                    if consecutive_failures >= CIRCUIT_BREAKER_THRESHOLD:\n                        logger.critical(f\"[CIRCUIT-BREAKER] {consecutive_failures} fallos consecutivos. \"\n                                        f\"Pipeline detenido. \u00daltimo tramo exitoso: {last_successful_chunk_end}\")\n                        raise Exception(f\"Circuit Breaker activado tras {consecutive_failures} fallos consecutivos.\")\n                    break\n\n                # Metadatos\n                data_payload = response.json().get('QueryResponse', {}).get(entity, [])\n                \n                for record in data_payload:\n                    \n                    record_last_updated = record.get('MetaData', {}).get('LastUpdatedTime', '')\n                    \n                    all_final_records.append({\n                        'id': record.get('Id'),\n                        'payload': record,\n                        'ingested_at_utc': datetime.now(timezone.utc),\n                        'extract_window_start_utc': chunk_start,\n                        'extract_window_end_utc': chunk_end,\n                        'page_number': (start_position // PAGE_SIZE) + 1,\n                        'page_size': PAGE_SIZE,\n                        'request_payload': query,\n                        'source_last_updated_utc': record_last_updated\n                    })\n                \n                pages_in_chunk += 1\n                records_in_chunk += len(data_payload)\n                \n                if len(data_payload) < PAGE_SIZE:\n                    more_data_in_chunk = False\n                else:\n                    start_position += PAGE_SIZE\n                    time.sleep(COURTESY_WAIT)\n\n            # Metricas\n            duration_chunk = round(time.time() - start_time_chunk, 2)\n            \n            if records_in_chunk == 0:\n                logger.warning(f\"[VOLUMETRY] ALERTA: Tramo {chunk_start} a {chunk_end} retorn\u00f3 0 registros. \"\n                               f\"Verificar si es esperado o hay problema de filtros/datos.\")\n            \n            logger.info(f\"[METRICS] Tramo Finalizado: \"\n                        f\"Ventana: [{chunk_start} - {chunk_end}] | \"\n                        f\"P\u00e1ginas: {pages_in_chunk} | \"\n                        f\"Registros: {records_in_chunk} | \"\n                        f\"Duraci\u00f3n: {duration_chunk}s\")\n            \n            last_successful_chunk_end = chunk_end\n            \n        except Exception as e:\n            consecutive_failures += 1\n            pipeline_failed = True\n            logger.error(f\"[CHUNK-ERROR] Error en tramo #{chunk_index} ({chunk_start}): {str(e)}\")\n            \n            if last_successful_chunk_end:\n                logger.critical(f\"[CHECKPOINT] \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\")\n                logger.critical(f\"[CHECKPOINT] PIPELINE INTERRUMPIDO EN TRAMO #{chunk_index}\")\n                logger.critical(f\"[CHECKPOINT] \u00daltimo tramo exitoso: #{chunk_index - 1}\")\n                logger.critical(f\"[CHECKPOINT] \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\")\n                logger.critical(f\"[CHECKPOINT] PARA REANUDAR, usar par\u00e1metro:\")\n                logger.critical(f\"[CHECKPOINT] resume_from = '{last_successful_chunk_end}'\")\n                logger.critical(f\"[CHECKPOINT] \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\")\n            else:\n                logger.critical(f\"[CHECKPOINT] PIPELINE FALL\u00d3 EN EL PRIMER TRAMO.\")\n            \n            logger.warning(f\"[RECOVERY] Retornando {len(all_final_records)} registros de tramos exitosos anteriores.\")\n            break\n\n        current_date = next_date\n\n    # Resumen final\n    total_duration = round(time.time() - total_start_time, 2)\n    \n    if pipeline_failed:\n        logger.warning(f\"[EXTRACTION-PARTIAL] === EXTRACCI\u00d3N PARCIAL (CON ERRORES) ===\")\n        logger.warning(f\"[EXTRACTION-PARTIAL] Tramos completados exitosamente: {chunk_index - 1}\")\n    else:\n        logger.info(f\"[EXTRACTION-COMPLETE] === EXTRACCI\u00d3N FINALIZADA EXITOSAMENTE ===\")\n    \n    logger.info(f\"[EXTRACTION-COMPLETE] Total registros: {len(all_final_records)}\")\n    logger.info(f\"[EXTRACTION-COMPLETE] Duraci\u00f3n total: {total_duration}s\")\n    logger.info(f\"[EXTRACTION-COMPLETE] Entidad: {entity}\")\n    logger.info(f\"[EXTRACTION-COMPLETE] Rango solicitado: {start_date_str} a {end_date_str}\")\n    if resume_from_str:\n        logger.info(f\"[EXTRACTION-COMPLETE] Reanudado desde checkpoint: {resume_from_str}\")\n    \n    if len(all_final_records) == 0:\n        logger.warning(\"[VOLUMETRY] No se extrajeron registros. Verificar rango de fechas y datos en QBO.\")\n    \n    df = pd.DataFrame(all_final_records)\n    \n    if not df.empty:\n        df.attrs['last_checkpoint'] = last_successful_chunk_end\n        df.attrs['pipeline_failed'] = pipeline_failed\n        df.attrs['original_fecha_fin'] = original_fecha_fin\n    \n    return df", "file_path": "/home/src/default_repo/data_loaders/invoices_data_loader.py", "language": "python", "type": "data_loader", "uuid": "invoices_data_loader"}, "/home/src/default_repo/data_exporters/invoices_data_exporter.py:data_exporter:python:home/src/default repo/data exporters/invoices data exporter": {"content": "from mage_ai.data_preparation.shared.secrets import get_secret_value\nimport psycopg2\nimport json\nimport time\nimport pandas as pd\nfrom datetime import datetime, timezone\nfrom dateutil import parser as date_parser\n\nMAX_DB_RETRIES = 3\nDB_RETRY_BACKOFF = 2\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\ndef get_db_connection_with_retry(db_params, logger):\n    retries = 0\n    while retries < MAX_DB_RETRIES:\n        try:\n            conn = psycopg2.connect(**db_params)\n            return conn\n        except psycopg2.OperationalError as e:\n            retries += 1\n            wait = (2 ** retries) * DB_RETRY_BACKOFF\n            logger.warning(f\"[DB-RETRY] Error de conexi\u00f3n: {str(e)}. Reintento {retries}/{MAX_DB_RETRIES} en {wait}s\")\n            time.sleep(wait)\n    raise Exception(f\"[DB-FAIL] No se pudo conectar a Postgres tras {MAX_DB_RETRIES} reintentos.\")\n\n\n@data_exporter\ndef export_data_to_postgres(df, *args, **kwargs):\n    logger = kwargs.get('logger')\n    start_time_load = time.time()\n    \n    entity = 'Invoice'\n    table_name = f\"qb_{entity.lower()}\"\n    schema_name = \"raw\"\n    \n    if df is None or df.empty:\n        logger.warning(f\"[VOLUMETRY] No hay datos para la entidad {table_name}. Fin de ejecuci\u00f3n.\")\n        return\n\n    try:\n        db_params = {\n            'host': get_secret_value('POSTGRES_HOST'),\n            'database': get_secret_value('POSTGRES_DB'),\n            'user': get_secret_value('POSTGRES_USER'),\n            'password': get_secret_value('POSTGRES_PASSWORD'),\n            'port': get_secret_value('POSTGRES_PORT')\n        }\n        conn = get_db_connection_with_retry(db_params, logger)\n        cur = conn.cursor()\n    except Exception as e:\n        logger.error(f\"[SECURITY/DB] Error al obtener secretos o conectar a Postgres: {str(e)}\")\n        raise e\n\n    try:\n        cur.execute(\"BEGIN;\")\n        cur.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema_name};\")\n        \n        create_table_query = f\"\"\"\n            CREATE TABLE IF NOT EXISTS {schema_name}.{table_name} (\n                id VARCHAR PRIMARY KEY,\n                payload JSONB,\n                ingested_at_utc TIMESTAMP WITH TIME ZONE,\n                extract_window_start_utc TIMESTAMP WITH TIME ZONE,\n                extract_window_end_utc TIMESTAMP WITH TIME ZONE,\n                page_number INT,\n                page_size INT,\n                request_payload TEXT,\n                source_last_updated_utc TIMESTAMP WITH TIME ZONE\n            );\n        \"\"\"\n        cur.execute(create_table_query)\n        conn.commit()\n        logger.info(f\"[DDL] Tabla {schema_name}.{table_name} creada/verificada exitosamente.\")\n    except Exception as e:\n        logger.error(f\"[DDL] Error creando infraestructura RAW: {str(e)}\")\n        conn.rollback()\n        raise e\n\n    try:\n        cur.execute(f\"SELECT COUNT(*) FROM {schema_name}.{table_name}\")\n        count_before = cur.fetchone()[0]\n    except:\n        count_before = 0\n\n    upsert_sql = f\"\"\"\n        INSERT INTO {schema_name}.{table_name} (\n            id, payload, ingested_at_utc, extract_window_start_utc, \n            extract_window_end_utc, page_number, page_size, request_payload,\n            source_last_updated_utc\n        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n        ON CONFLICT (id) DO UPDATE SET\n            payload = EXCLUDED.payload,\n            ingested_at_utc = EXCLUDED.ingested_at_utc,\n            extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n            extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n            page_number = EXCLUDED.page_number,\n            page_size = EXCLUDED.page_size,\n            request_payload = EXCLUDED.request_payload,\n            source_last_updated_utc = EXCLUDED.source_last_updated_utc;\n    \"\"\"\n\n    rows_processed = 0\n    rows_with_temporal_issues = 0\n    rows_skipped_null_id = 0\n    chunk_metrics = {}\n    \n    try:\n        for _, row in df.iterrows():\n            if not row['id'] or pd.isna(row['id']):\n                logger.error(f\"[VALIDATION] Registro con ID nulo omitido en exporter.\")\n                rows_skipped_null_id += 1\n                continue\n            \n            chunk_key = f\"{row['extract_window_start_utc']}|{row['extract_window_end_utc']}\"\n            if chunk_key not in chunk_metrics:\n                chunk_metrics[chunk_key] = {\n                    'count': 0,\n                    'window_start': row['extract_window_start_utc'],\n                    'window_end': row['extract_window_end_utc']\n                }\n            chunk_metrics[chunk_key]['count'] += 1\n            \n            ingested_at = row['ingested_at_utc']\n            window_end_str = row['extract_window_end_utc']\n            source_updated = row.get('source_last_updated_utc', '')\n            \n            try:\n                window_end_dt = date_parser.parse(window_end_str) if isinstance(window_end_str, str) else window_end_str\n                if ingested_at.tzinfo is None:\n                    ingested_at = ingested_at.replace(tzinfo=timezone.utc)\n                if window_end_dt.tzinfo is None:\n                    window_end_dt = window_end_dt.replace(tzinfo=timezone.utc)\n                    \n                if ingested_at < window_end_dt:\n                    rows_with_temporal_issues += 1\n                    logger.debug(f\"[TEMPORAL-WARNING] Registro {row['id']}: ingested_at < extract_window_end\")\n            except Exception as parse_error:\n                logger.debug(f\"[TEMPORAL-PARSE] No se pudo validar temporalidad para {row['id']}: {parse_error}\")\n            \n            source_updated_ts = None\n            if source_updated:\n                try:\n                    source_updated_ts = date_parser.parse(source_updated)\n                except:\n                    source_updated_ts = None\n            \n            retry_count = 0\n            while retry_count < MAX_DB_RETRIES:\n                try:\n                    cur.execute(upsert_sql, (\n                        str(row['id']), \n                        json.dumps(row['payload']), \n                        row['ingested_at_utc'],\n                        row['extract_window_start_utc'], \n                        row['extract_window_end_utc'],\n                        row['page_number'], \n                        row['page_size'], \n                        row['request_payload'],\n                        source_updated_ts\n                    ))\n                    rows_processed += 1\n                    break\n                except psycopg2.OperationalError as e:\n                    retry_count += 1\n                    if retry_count >= MAX_DB_RETRIES:\n                        raise e\n                    logger.warning(f\"[DB-RETRY] Error en INSERT, reintentando... {retry_count}/{MAX_DB_RETRIES}\")\n                    time.sleep(DB_RETRY_BACKOFF * retry_count)\n\n                    try:\n                        conn = get_db_connection_with_retry(db_params, logger)\n                        cur = conn.cursor()\n                    except:\n                        pass\n        \n        conn.commit()\n        logger.info(f\"[LOAD] Upsert exitoso: {rows_processed} filas procesadas en {table_name}.\")\n        \n    except Exception as e:\n        logger.error(f\"[LOAD] Fallo en la carga de datos: {str(e)}\")\n        conn.rollback()\n        raise e\n\n    # Metricas\n    try:\n        cur.execute(f\"SELECT COUNT(*) FROM {schema_name}.{table_name}\")\n        count_after = cur.fetchone()[0]\n        \n        new_inserts = count_after - count_before\n        updates = rows_processed - new_inserts if rows_processed > new_inserts else 0\n\n        if new_inserts < 0:\n            new_inserts = 0\n            updates = rows_processed\n        omitted = len(df) - rows_processed\n        \n        logger.info(\"--- REPORTE DE CALIDAD ---\")\n        logger.info(f\"[QUALITY] Entidad: {table_name}\")\n        logger.info(f\"[QUALITY] Registros en DataFrame: {len(df)}\")\n        logger.info(f\"[QUALITY] Total registros en tabla (antes): {count_before}\")\n        logger.info(f\"[QUALITY] Total registros en tabla (despu\u00e9s): {count_after}\")\n        logger.info(f\"[METRICS-GRANULAR] Nuevas inserciones: {new_inserts}\")\n        logger.info(f\"[METRICS-GRANULAR] Actualizaciones (updates): {updates}\")\n        logger.info(f\"[METRICS-GRANULAR] Omitidos/errores: {omitted}\")\n        logger.info(f\"[IDEMPOTENCY] Upsert aplicado correctamente sobre PK 'id'.\")\n        \n        if rows_with_temporal_issues > 0:\n            logger.warning(f\"[TEMPORAL-QUALITY] {rows_with_temporal_issues} registros con posibles \"\n                           f\"inconsistencias temporales (ingested_at < extract_window_end).\")\n        \n        if rows_skipped_null_id > 0:\n            logger.warning(f\"[INTEGRITY] {rows_skipped_null_id} registros omitidos por ID nulo.\")\n        \n        logger.info(\"--- VOLUMETR\u00cdA POR TRAMO ---\")\n        for chunk_key, metrics in chunk_metrics.items():\n            logger.info(f\"[CHUNK-VOLUMETRY] Ventana: [{metrics['window_start']} - {metrics['window_end']}] | \"\n                        f\"Registros: {metrics['count']}\")\n\n            if metrics['count'] == 0:\n                logger.warning(f\"[VOLUMETRY] ALERTA: Tramo vac\u00edo detectado: {chunk_key}\")\n        logger.info(f\"[VOLUMETRY] Total tramos procesados: {len(chunk_metrics)}\")\n        \n        if rows_processed == 0 and not df.empty:\n            logger.warning(\"[QUALITY] ALERTA: El DataFrame ten\u00eda datos pero no se proces\u00f3 nada en Postgres.\")\n\n    except Exception as e:\n        logger.warning(f\"[QUALITY] No se pudo generar reporte de volumetr\u00eda: {str(e)}\")\n\n    try:\n        pipeline_failed = False\n        if hasattr(df, 'attrs'):\n            pipeline_failed = df.attrs.get('pipeline_failed', False)\n        \n        if pipeline_failed:\n            logger.warning(f\"[EXPORTER] Datos de extracci\u00f3n parcial exportados exitosamente.\")\n            logger.warning(f\"[EXPORTER] Revisar logs del Loader para instrucciones de reanudaci\u00f3n.\")\n        else:\n            logger.info(f\"[EXPORTER] Pipeline completado exitosamente.\")\n    except Exception as e:\n        logger.warning(f\"[EXPORTER] No se pudo verificar estado del pipeline: {str(e)}\")\n    \n    finally:\n        cur.close()\n        conn.close()\n\n    # Resumen final\n    duration = round(time.time() - start_time_load, 2)\n    logger.info(\"--- RESUMEN FINAL ---\")\n    logger.info(f\"Registros procesados: {rows_processed}\")\n    logger.info(f\"Nuevos: {new_inserts} | Actualizados: {updates} | Omitidos: {omitted}\")\n    logger.info(f\"Duraci\u00f3n: {duration} segundos\")\n    logger.info(f\"Coherencia Temporal: Marcas registradas en UTC\")\n    logger.info(\"--------------------------------------------\")", "file_path": "/home/src/default_repo/data_exporters/invoices_data_exporter.py", "language": "python", "type": "data_exporter", "uuid": "invoices_data_exporter"}, "/home/src/default_repo/data_loaders/honorable_flower.py:data_loader:python:home/src/default repo/data loaders/honorable flower": {"content": "from mage_ai.orchestration.db import db_connection\nfrom mage_ai.orchestration.db.models import Secret\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport datetime\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n@data_loader\ndef test_hardcore_rotation(*args, **kwargs):\n    # 1. Par\u00e1metros de prueba\n    # ASEG\u00daRATE de haber creado este secreto en Settings -> Secrets antes\n    target_secret = 'TEST_ROTATION_TOKEN' \n    nuevo_valor = f\"TOKEN_GENERADO_{datetime.datetime.now().strftime('%H%M%S')}\"\n    \n    print(f\"--- INICIANDO PRUEBA DE ROTACI\u00d3N ---\")\n    print(f\"Buscando secreto: {target_secret}\")\n    \n    try:\n        # 2. Abrir sesi\u00f3n de base de datos interna de Mage\n        with db_connection.session_scope() as session:\n            print(\"Conexi\u00f3n a la DB de Mage abierta exitosamente.\")\n            \n            # 3. Buscar el registro del secreto\n            secret_record = session.query(Secret).filter(Secret.name == target_secret).first()\n            \n            if secret_record:\n                print(f\"Secreto encontrado. Valor actual: {secret_record.value}\")\n                print(f\"Intentando cambiar a: {nuevo_valor}\")\n                \n                # 4. Actualizar el valor\n                secret_record.value = nuevo_valor\n                session.add(secret_record)\n                \n                print(\"Cambio aplicado en la sesi\u00f3n. Esperando commit...\")\n            else:\n                print(f\"ERROR: El secreto '{target_secret}' no existe en Mage Secrets.\")\n                return {\"status\": \"not_found\"}\n\n        # Al salir del bloque 'with', Mage hace el commit autom\u00e1tico\n        print(\"Commit realizado con \u00e9xito.\")\n\n        # 5. Verificaci\u00f3n final usando la funci\u00f3n est\u00e1ndar\n        verificacion = get_secret_value(target_secret)\n        print(f\"VERIFICACI\u00d3N FINAL: El nuevo valor recuperado es: {verificacion}\")\n        \n        return {\n            \"status\": \"success\",\n            \"valor_nuevo\": verificacion,\n            \"coincide\": verificacion == nuevo_valor\n        }\n\n    except Exception as e:\n        print(f\"FALLO CR\u00cdTICO: {str(e)}\")\n        # Importante para que el bloque se ponga en ROJO si falla\n        raise e", "file_path": "/home/src/default_repo/data_loaders/honorable_flower.py", "language": "python", "type": "data_loader", "uuid": "honorable_flower"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}